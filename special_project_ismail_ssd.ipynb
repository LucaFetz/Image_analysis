{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2019:][iapr2019] Special project\n",
    "\n",
    "**Group members:**\n",
    "    1- first name and last name,\n",
    "    2- first name and last name,\n",
    "    3- first name and last name\n",
    "\n",
    "**Due date:** 30.05.2019\n",
    "\n",
    "[iapr2019]: https://github.com/LTS5/iapr-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Please find the description of this special project via [this link].\n",
    "\n",
    "[this link]: https://github.com/LTS5/iapr-2019/blob/master/project/special_project_description.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "#  Folder extraction\n",
    "data_base_path = os.path.join(os.pardir, 'datasets')\n",
    "data_folder = 'project-data'\n",
    "data_projet_path  = os.path.join(data_base_path,data_folder)\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "if not os.path.exists(data_projet_path):\n",
    "    with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "        tar.extractall(path=data_base_path)\n",
    "\n",
    "# Data project path\n",
    "img_projet_path   = os.path.join(data_projet_path,'images')\n",
    "annot_projet_path = os.path.join(data_projet_path,'annotations')\n",
    "# Read files that contains names\n",
    "val_name_file  ='validation.txt'\n",
    "test_name_file ='test.txt'\n",
    "train_name_file='train.txt'\n",
    "\n",
    "# Validation\n",
    "val_img_fn_path = os.path.join(data_projet_path,val_name_file)\n",
    "val_img_names= [line.rstrip('\\n') for line in open(val_img_fn_path)]\n",
    "val_img_fn   = [line +'.jpg' for line in val_img_names]\n",
    "val_img_path = [os.path.join(os.path.join(img_projet_path,'validation'),line) for line in val_img_fn]\n",
    "val_ic = skimage.io.imread_collection(val_img_path)\n",
    "print('Number of validation images: ', len(val_ic))\n",
    "\n",
    "# Train\n",
    "tr_img_fn_path = os.path.join(data_projet_path,train_name_file)\n",
    "tr_img_names= [line.rstrip('\\n') for line in open(tr_img_fn_path)]\n",
    "tr_img_fn   = [line +'.jpg' for line in tr_img_names]\n",
    "tr_img_path = [os.path.join(os.path.join(img_projet_path,'train'),line) for line in tr_img_fn]\n",
    "tr_ic = skimage.io.imread_collection(tr_img_path)\n",
    "print('Number of validation images: ', len(tr_ic))\n",
    "\n",
    "# Test\n",
    "tst_img_fn_path = os.path.join(data_projet_path,test_name_file)\n",
    "tst_img_names= [line.rstrip('\\n') for line in open(tst_img_fn_path)]\n",
    "tst_img_fn   = [line +'.jpg' for line in tst_img_names]\n",
    "tst_img_path = [os.path.join(os.path.join(img_projet_path,'test'),line) for line in tst_img_fn]\n",
    "tst_ic = skimage.io.imread_collection(tst_img_path)\n",
    "print('Number of test images: ', len(tst_ic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymin').text)),\n",
    "                              int(float(bbox.find('xmax').text))-int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymax').text))-int(float(bbox.find('ymin').text))]\n",
    "        objects.append(obj_struct)\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Xml annotations\n",
    "val_xml_path = os.path.join(annot_projet_path, 'validation')\n",
    "val_xmls = [parse_file(os.path.join(val_xml_path,name + '.xml')) for name in val_img_names]\n",
    "\n",
    "tr_xml_path = os.path.join(annot_projet_path,'train')\n",
    "tr_xmls = [parse_file(os.path.join(tr_xml_path,name + '.xml')) for name in tr_img_names]\n",
    "\n",
    "tst_xml_path = os.path.join(annot_projet_path,'test')\n",
    "tst_xmls = [parse_file(os.path.join(tst_xml_path,name + '.xml')) for name in tst_img_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "for ax, im, nm, annotations in zip(axes.ravel(), tst_ic, tst_img_names[0:6], tst_xmls):\n",
    "    # Iterate over annotations\n",
    "    for anno in annotations:\n",
    "        rect = patches.Rectangle((anno['bbox'][0], anno['bbox'][1]), anno['bbox'][2], anno['bbox'][3],\n",
    "                linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding varroas by segmentation\n",
    "Add your implementation for ''**detect_by_segmentation**'' function. Please make sure the input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from skimage.filters import threshold_li\n",
    "from skimage.color   import rgb2gray\n",
    "from skimage.measure import label\n",
    "\n",
    "from skimage.morphology import erosion, dilation, opening, closing, white_tophat\n",
    "from skimage.morphology import black_tophat, skeletonize, convex_hull_image\n",
    "from skimage.morphology import disk\n",
    "\n",
    "def pipeline(rgb_input_img):\n",
    "    \"\"\" Input : RGB image of varroa infection\n",
    "        Ouput : Binary image of detected varroa \n",
    "        Function : Process the image in order to keep the relevant informations \"\"\"\n",
    "    # Principal pipeline \n",
    "\n",
    "    # Preprocessing\n",
    "    \n",
    "    # Contrast enhancement\n",
    "    sigmoid_p = 0.5\n",
    "    rgb_input_img = skimage.exposure.adjust_sigmoid(rgb_input_img, cutoff=sigmoid_p, gain=10, inv=False)    \n",
    "    \n",
    "    # Color Selection\n",
    "    single_canal = False\n",
    "    \n",
    "    input_img = rgb2gray(rgb_input_img)\n",
    "    \n",
    "    # Morphology operations\n",
    "    \n",
    "    struct_elem = disk(4)\n",
    "    morph_img   = dilation(input_img, struct_elem)# Remove waste on images\n",
    "    struct_elem = disk(4)\n",
    "    morph_img   = erosion(morph_img,struct_elem) # Give proportion again to varroa\n",
    "    \n",
    "    # Thresholding\n",
    "    \n",
    "    # Li method thres.\n",
    "    li_thres = threshold_li(morph_img)\n",
    "        \n",
    "    # Binarization\n",
    "    bin_min  = morph_img < li_thres\n",
    "\n",
    "\n",
    "    # Plot the results\n",
    "    plot_processing = False\n",
    "    if True == plot_processing:\n",
    "        fig, axes = plt.subplots(ncols=3, figsize=(15, 15))\n",
    "        ax = axes.ravel()\n",
    "\n",
    "        ax[0].imshow(input_img)\n",
    "        ax[0].set_xlabel('Orig.')\n",
    "\n",
    "        ax[1].imshow(morph_img)\n",
    "        ax[1].set_xlabel('Morph. processing.')\n",
    "\n",
    "        ax[2].imshow(bin_min)\n",
    "        ax[2].set_xlabel('Minimal thres.')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Assign the output to final processing\n",
    "    output = bin_min\n",
    "    \n",
    "    return output\n",
    "\n",
    "def label_varroa_region(input_image):\n",
    "    \"\"\" Input  : Binary image of detected varroa\n",
    "        Output : Multiples labellised regions of the image\"\"\"\n",
    "    output_labels,num_labels = label(input_image,return_num=True,connectivity=None)\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import regionprops\n",
    "\n",
    "def xml_annotation_to_bbox(xml_annotation):\n",
    "    \"\"\" Convert an xml annotation to a list of bbox tuples\"\"\"\n",
    "    bbox_list = []\n",
    "    for elem in xml_annotation:\n",
    "        bbox_coords = elem['bbox']\n",
    "        bbox_tuple  = tuple(bbox_coords)\n",
    "        bbox_list.append(bbox_tuple)\n",
    "    \n",
    "    return bbox_list\n",
    "\n",
    "\n",
    "def iou_bbox(gt_reg,pred_reg):\n",
    "    \"\"\" \n",
    "        Compute the IoU between two region with their bounding boxes\n",
    "        \n",
    "        Insipired from : https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "        \n",
    "        Parameters\n",
    "        ----- \n",
    "        pred_reg format : (MIN_ROW,MIN_COL,MAX_ROW,MAX_COL)\n",
    "        pred_reg pixels in bounding box are in half-open interval [min_row; max_row) and [min_col; max_col).\n",
    "        gt_reg h-format : (x, y, width, heigth) in some cartesian form\n",
    "        gt_reg format   : (MIN_COL,MIN_ROW, MAX_ROW-MIN_ROW, MAX_COL-MIN_COL)\n",
    "        \n",
    "        Return\n",
    "        -----\n",
    "        float val in [0,1]\n",
    "    \"\"\"\n",
    "    # x:rows and y:colummns are (0,0) at right corner\n",
    "    \n",
    "    # Harmonize the coordinates systems of the bounding boxes\n",
    "    g_y_min,g_x_min,diff_x,diff_y   = gt_reg\n",
    "    g_y_max = g_y_min + diff_y \n",
    "    g_x_max = g_x_min + diff_x\n",
    "    \n",
    "    # In ground truth max_values are inside the bbox but in label are outside \n",
    "    p_x_min,p_y_min,p_x_max,p_y_max = pred_reg\n",
    "    # Correction of max values\n",
    "    p_x_max = p_x_max - 1\n",
    "    p_y_max = p_y_max - 1\n",
    "    \n",
    "    # Determine the (x,y) coordinates of the intersection rectangle\n",
    "    i_x_min = max(p_x_min,g_x_min)\n",
    "    i_y_min = max(p_y_min,g_y_min)\n",
    "    i_x_max = min(p_x_max,g_x_max)\n",
    "    i_y_max = min(p_y_max,g_y_max)\n",
    "    \n",
    "    # Compute the intersection area (robust to non-matching rect.s)\n",
    "    inter_area = max(0,i_x_max -  i_x_min + 1) * max(0,i_y_max - i_y_min + 1)\n",
    "    \n",
    "    \n",
    "    # Compute the area of both the prediction and the ground-truth\n",
    "    pred_rect = (p_x_max-p_x_min+1)*(p_y_max-p_y_min+1) \n",
    "    gt_rect   = (g_x_max-g_x_min+1)*(g_y_max-g_y_min+1)\n",
    "\n",
    "    debug_print = False\n",
    "    \n",
    "    if True == debug_print:\n",
    "        print(\" GT : {}:{}/{}:{}\".format(g_x_min,g_y_min,g_x_max,g_y_max))\n",
    "        print(\" PR : {}:{}/{}:{}\".format(p_x_min,p_y_min,p_x_max,p_y_max))\n",
    "        print(\"INTER : {}\".format(inter_area))\n",
    "        print(\"PRED_AREA : {}\".format(pred_rect))\n",
    "        print(\"GT   AREA : {}\".format(gt_rect))\n",
    "    \n",
    "    # Compute the intersection over union\n",
    "    iou_val = inter_area/(gt_rect+pred_rect-inter_area)\n",
    "    return iou_val\n",
    "    \n",
    "\n",
    "def iou_bbox2(gt_reg,pred_reg):\n",
    "    \"\"\" \n",
    "        Compute the IoU between two region with their bounding boxes\n",
    "         \n",
    "        Parameters\n",
    "        ----- \n",
    "        pred_reg format : (MIN_COL,MIN_ROW, MAX_ROW-MIN_ROW, MAX_COL-MIN_COL)\n",
    "        pred_reg pixels in bounding box are in half-open interval [min_row; max_row) and [min_col; max_col).\n",
    "        gt_reg h-format : (x, y, width, heigth) in some cartesian form\n",
    "        gt_reg format   : (MIN_COL,MIN_ROW, MAX_ROW-MIN_ROW, MAX_COL-MIN_COL)\n",
    "        \n",
    "        Return\n",
    "        -----\n",
    "        float val in [0,1]\n",
    "    \"\"\"\n",
    "    # x:col and y:rows are (0,0) at right corner\n",
    "    \n",
    "    # Harmonize the coordinates systems of the bounding boxes\n",
    "    g_x_min,g_y_min,diff_x,diff_y = gt_reg\n",
    "    g_y_max = g_y_min + diff_y \n",
    "    g_x_max = g_x_min + diff_x\n",
    "    \n",
    "    # In ground truth max_values are inside the bbox but in label are outside \n",
    "    p_x_min,p_y_min,p_diff_x,p_diff_y = pred_reg\n",
    "    # Correction of max values\n",
    "    p_x_max = p_x_min + p_diff_x\n",
    "    p_y_max = p_y_min + p_diff_y\n",
    "    \n",
    "    # Determine the (x,y) coordinates of the intersection rectangle\n",
    "    i_x_min = max(p_x_min,g_x_min)\n",
    "    i_y_min = max(p_y_min,g_y_min)\n",
    "    i_x_max = min(p_x_max,g_x_max)\n",
    "    i_y_max = min(p_y_max,g_y_max)\n",
    "    \n",
    "    # Compute the intersection area (robust to non-matching rect.s)\n",
    "    inter_area = max(0,i_x_max -  i_x_min + 1) * max(0,i_y_max - i_y_min + 1)\n",
    "    \n",
    "    \n",
    "    # Compute the area of both the prediction and the ground-truth\n",
    "    pred_rect = (p_x_max-p_x_min+1)*(p_y_max-p_y_min+1) \n",
    "    gt_rect   = (g_x_max-g_x_min+1)*(g_y_max-g_y_min+1)\n",
    "\n",
    "    debug_print = False\n",
    "    \n",
    "    if True == debug_print:\n",
    "        print(\" GT : {}:{}/{}:{}\".format(g_x_min,g_y_min,g_x_max,g_y_max))\n",
    "        print(\" PR : {}:{}/{}:{}\".format(p_x_min,p_y_min,p_x_max,p_y_max))\n",
    "        print(\"INTER : {}\".format(inter_area))\n",
    "        print(\"PRED_AREA : {}\".format(pred_rect))\n",
    "        print(\"GT   AREA : {}\".format(gt_rect))\n",
    "    \n",
    "    # Compute the intersection over union\n",
    "    iou_val = inter_area/(gt_rect+pred_rect-inter_area)\n",
    "    return iou_val\n",
    "\n",
    "\n",
    "def detection_statistics(pred_image,gt_labels,threshold):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a tuple composed by pred_region_stats and gt_region_stats\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    pred_image : labeled image\n",
    "    gt_labels  : list of bbox tuples per ground truth region\n",
    "    threshold  : IoU threshold\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    (pred_region_stats,gt_region_stats)\n",
    "    \n",
    "    pred_region_stats :\n",
    "    a list of size of number of predicted region having the following status depending on the IoU criteria\n",
    "    3 : True  positive (region is correct)\n",
    "    1 : False positive (region doesn't exist in ground truth or don't match enough)\n",
    "    \n",
    "    gt_region_stats :\n",
    "    a list of size of number of ground truth region having the following status depending on the IoU criteria\n",
    "    0 : Region being detected\n",
    "    -3: False negative (existing region not detected)\n",
    "    \"\"\"\n",
    "    debug_print = False # Variable to enable debuging information  \n",
    "        \n",
    "    props_list = regionprops(pred_image) # Properties of the labelled image\n",
    "    pred_region_status = np.zeros(len(props_list)) # Select between true or false positive\n",
    "    pred_region_argmax = np.zeros(len(props_list)) # Array that stores the best IoU idx per predicted region\n",
    "    gt_region_status = np.zeros(len(gt_labels))    # Store if ground truth region is being detected.\n",
    "    \n",
    "    # Rows : Gt regions,Cols : Predicted regions\n",
    "    iou_array = np.zeros((len(gt_labels),len(props_list)),dtype=np.float64)     \n",
    "    \n",
    "    # Compute all the IoU bruteforce method\n",
    "    for idx_a,a in enumerate(gt_labels):\n",
    "        for idx_b,b in enumerate(props_list):\n",
    "            b_bbox = b.bbox\n",
    "            \n",
    "            # Debug info begin\n",
    "            if True == debug_print:\n",
    "                print(\"gt_idx {} / reg_idx {} \".format(idx_a,idx_b))\n",
    "                print(\"Bbox coords ----------\")\n",
    "                print(\"GT : {}\\nPRED : {}\".format(a,b_bbox))\n",
    "            # Debug info end\n",
    "            iou_array[idx_a,idx_b] = iou_bbox(a,b_bbox)\n",
    "    \n",
    "    # Debug info begin\n",
    "    if True == debug_print:\n",
    "        print(\"--------------\")\n",
    "        print(\"mtx size : {}\",iou_array.shape)\n",
    "        print(iou_array)\n",
    "        print(\"--------------\")\n",
    "    # Debug info end\n",
    "    \n",
    "    # Compute the false positive (pred region which IoU equals 0 over the ground truth is equal to zero)\n",
    "    false_pos_idx = [iou_array.sum(0) == 0]\n",
    "    pred_region_status[false_pos_idx] = 1.0 # False positive\n",
    "    pred_region_argmax[false_pos_idx] = -1.0 # False positive then no assignation to a region\n",
    "    \n",
    "    # Compute the true positive depending on the threshold\n",
    "    for pred_idx,elem in enumerate(pred_region_status):\n",
    "        if elem != 1.0:\n",
    "            gt_idx = iou_array[:,pred_idx].argmax() # Get the maximum IoU\n",
    "            # Check if IoU is above the threshold\n",
    "            if iou_array[gt_idx,pred_idx] >= threshold:\n",
    "                pred_region_status[pred_idx] = 3. # True positive\n",
    "                pred_region_argmax[pred_idx] = gt_idx # Assign the predicted region to a ground truth region\n",
    "            else:\n",
    "                pred_region_status[pred_idx] = 1.   # False positive\n",
    "                pred_region_argmax[pred_idx] = -1.0 # False positive then no assignation to a region       \n",
    "                \n",
    "    # Compute the false negative\n",
    "    for idx,elem in enumerate(gt_region_status):\n",
    "        if idx not in pred_region_argmax: # If no predicted region has been assigned to a ground truth region\n",
    "            gt_region_status[idx] = -3.0 # False negative\n",
    "    \n",
    "    return (pred_region_status,gt_region_status)\n",
    "\n",
    "def detection_statistics_2(pred_labels,gt_labels,threshold):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a tuple composed by pred_region_stats and gt_region_stats\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    pred_labels : list of bbox tuples per predicted region\n",
    "    gt_labels   : list of bbox tuples per ground truth region\n",
    "    threshold   : IoU threshold\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    (pred_region_stats,gt_region_stats)\n",
    "    \n",
    "    pred_region_stats :\n",
    "    a list of size of number of predicted region having the following status depending on the IoU criteria\n",
    "    3 : True  positive (region is correct)\n",
    "    1 : False positive (region doesn't exist in ground truth or don't match enough)\n",
    "    \n",
    "    gt_region_stats :\n",
    "    a list of size of number of ground truth region having the following status depending on the IoU criteria\n",
    "    0 : Region being detected\n",
    "    -3: False negative (existing region not detected)\n",
    "    \"\"\"\n",
    "    debug_print = False # Variable to enable debuging information  \n",
    "        \n",
    "    pred_region_status = np.zeros(len(pred_labels)) # Select between true or false positive\n",
    "    pred_region_argmax = np.zeros(len(pred_labels)) # Array that stores the best IoU idx per predicted region\n",
    "    gt_region_status = np.zeros(len(gt_labels))    # Store if ground truth region is being detected.\n",
    "    \n",
    "    # Rows : Gt regions,Cols : Predicted regions\n",
    "    iou_array = np.zeros((len(gt_labels),len(pred_labels)),dtype=np.float64)     \n",
    "    \n",
    "    # Compute all the IoU bruteforce method\n",
    "    for idx_a,a in enumerate(gt_labels):\n",
    "        for idx_b,b in enumerate(pred_labels):\n",
    "            # Debug info begin\n",
    "            if True == debug_print:\n",
    "                print(\"gt_idx {} / reg_idx {} \".format(idx_a,idx_b))\n",
    "                print(\"Bbox coords ----------\")\n",
    "                print(\"GT : {}\\nPRED : {}\".format(a,b))\n",
    "            # Debug info end\n",
    "            iou_array[idx_a,idx_b] = iou_bbox2(a,b)\n",
    "    \n",
    "    # Debug info begin\n",
    "    if True == debug_print:\n",
    "        print(\"--------------\")\n",
    "        print(\"mtx size : {}\",iou_array.shape)\n",
    "        print(iou_array)\n",
    "        print(\"--------------\")\n",
    "    # Debug info end\n",
    "    \n",
    "    # Compute the false positive (pred region which IoU equals 0 over the ground truth is equal to zero)\n",
    "    false_pos_idx = [iou_array.sum(0) == 0]\n",
    "    pred_region_status[false_pos_idx] = 1.0 # False positive\n",
    "    pred_region_argmax[false_pos_idx] = -1.0 # False positive then no assignation to a region\n",
    "    \n",
    "    # Compute the true positive depending on the threshold\n",
    "    for pred_idx,elem in enumerate(pred_region_status):\n",
    "        if elem != 1.0:\n",
    "            gt_idx = iou_array[:,pred_idx].argmax() # Get the maximum IoU\n",
    "            # Check if IoU is above the threshold\n",
    "            if iou_array[gt_idx,pred_idx] >= threshold:\n",
    "                pred_region_status[pred_idx] = 3. # True positive\n",
    "                pred_region_argmax[pred_idx] = gt_idx # Assign the predicted region to a ground truth region\n",
    "            else:\n",
    "                pred_region_status[pred_idx] = 1.   # False positive\n",
    "                pred_region_argmax[pred_idx] = -1.0 # False positive then no assignation to a region       \n",
    "                \n",
    "    # Compute the false negative\n",
    "    for idx,elem in enumerate(gt_region_status):\n",
    "        if idx not in pred_region_argmax: # If no predicted region has been assigned to a ground truth region\n",
    "            gt_region_status[idx] = -3.0 # False negative\n",
    "    \n",
    "    return (pred_region_status,gt_region_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(region_stats,gt_stats):\n",
    "    \"\"\" Compute the precision of the image detection tp/tp+fp\"\"\"\n",
    "    unique,counts = np.unique(region_stats, return_counts=True) # Get the list of number and # of occurences\n",
    "    stats_dict = dict(zip(unique,counts))\n",
    "    tp = stats_dict.get(3.0,0.0) # Get the number of true positive\n",
    "    fp = stats_dict.get(1.0,0.0) # Get the number of false positive \n",
    "    \n",
    "    # If nothing is detected then return zero (avoid numerical error)\n",
    "    if 0.0 == tp+fp:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return tp/(tp+fp)\n",
    "\n",
    "def recall(region_stats,gt_stats):\n",
    "    \"\"\" Compute the recall of the image detection tp/tp+fn\"\"\"\n",
    "    \n",
    "    unique,counts = np.unique(region_stats, return_counts=True) # Get the list of number and # of occurences\n",
    "    stats_dict = dict(zip(unique,counts))\n",
    "    tp = stats_dict.get(3.0,0.0) # Get the number of true positive\n",
    "    \n",
    "    unique,counts = np.unique(gt_stats, return_counts=True) # Get the list of number and # of occurences\n",
    "    stats_dict = dict(zip(unique,counts))\n",
    "    fn = stats_dict.get(-3.0,0.0) # Get the number of false negative \n",
    "    \n",
    "    # If nothing is detected then return zero (avoid numerical error)\n",
    "    if 0.0 == tp+fn :\n",
    "        return 0.0\n",
    "    else:\n",
    "        return tp/(tp+fn)\n",
    "    \n",
    "    \n",
    "\n",
    "def f1_score(region_stats,gt_stats):\n",
    "    \"\"\"\"\"\"\n",
    "    rec  = recall(region_stats,gt_stats)\n",
    "    prec = precision(region_stats,gt_stats)\n",
    "    \n",
    "    # If nothing is detected then return zero (avoid numerical error)\n",
    "    if 0. == prec or 0. == rec:\n",
    "        f1_val = 0.0\n",
    "    else:\n",
    "        f1_val = 2*prec*rec/(rec+prec)\n",
    "    print(\"Stats : Rec : {} / Prec : {} / F1-score : {}\".format(rec,prec,f1_val))\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_segmentation(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "    \n",
    "    bin_img = pipeline(img)\n",
    "    label_img = label_varroa_region(bin_img)\n",
    "    props_list = regionprops(label_img) # Properties of the labelled image\n",
    "    bbox_list = []\n",
    "    for elem in props_list:\n",
    "            min_r,min_c,max_r,max_c = elem.bbox\n",
    "            bbox_list.append((min_c,min_r,max_c-min_c,max_r-min_r))\n",
    "                             \n",
    "    return bbox_list\n",
    "    #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your code\n",
    "\n",
    "\n",
    "# Process the images and print some statistics\n",
    "for idx,img in enumerate(tst_ic):\n",
    "    bin_img = pipeline(img)\n",
    "    label_img = label_varroa_region(bin_img)\n",
    "    pred_labels = detect_by_segmentation(img) \n",
    "    gt_labels   = xml_annotation_to_bbox(tst_xmls[idx])\n",
    "    # Proceed to gather statistics\n",
    "    print(\"Image N° {}\".format(idx))\n",
    "    thres_list =  [0.3]\n",
    "    for thres in thres_list:\n",
    "        print(\"IoU threshold : {}\".format(thres))\n",
    "        print(\"Classic method\")\n",
    "        reg_stats,gt_stats = detection_statistics(label_img,gt_labels,thres)\n",
    "        f1_score(reg_stats,gt_stats)\n",
    "        print(\"Rewritten method\")\n",
    "        reg_stats,gt_stats = detection_statistics_2(pred_labels,gt_labels,thres)\n",
    "        f1_score(reg_stats,gt_stats)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using MLP and CNNs\n",
    "\n",
    "Add your implementation for the third part. Feel free to add your desirable functions, but please make sure you have proper functions for the final detection, where their input and output follows the same format as the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'train.txt': 100%|█████████████████████████████████████████████| 800/800 [00:02<00:00, 299.34it/s]\n",
      "Processing image set 'validation.txt': 100%|████████████████████████████████████████| 150/150 [00:00<00:00, 283.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# The directories that contain the images.\n",
    "varroa_train_images_dir  = '../datasets/project-data/images/train/'\n",
    "varroa_test_images_dir   = '../datasets/project-data/images/test/'\n",
    "varroa_val_images_dir    = '../datasets/project-data/images/validation/'\n",
    "\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "varroa_train_annotations_dir = '../datasets/project-data/annotations/train/'\n",
    "varroa_test_annotations_dir  = '../datasets/project-data/annotations/test/'\n",
    "varroa_val_annotations_dir   = '../datasets/project-data/annotations/validation/'\n",
    "\n",
    "\n",
    "# The paths to the image sets.\n",
    "varroa_train_image_set_filename = '../datasets/project-data/train.txt'\n",
    "varroa_test_image_set_filename  = '../datasets/project-data/test.txt'\n",
    "varroa_val_image_set_filename   = '../datasets/project-data/validation.txt'\n",
    "\n",
    "\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'Varroa']\n",
    "\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[varroa_train_images_dir],\n",
    "                        image_set_filenames=[varroa_train_image_set_filename],\n",
    "                        annotations_dirs=[varroa_train_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False,\n",
    "                        verbose=True)\n",
    "\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[varroa_val_images_dir],\n",
    "                        image_set_filenames=[varroa_val_image_set_filename],\n",
    "                        annotations_dirs=[varroa_val_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 480 # Height of the input images\n",
    "img_width = 480  # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "intensity_mean = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "intensity_range = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "n_classes = 1 # Number of positive classes\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "aspect_ratios = [0.5, 1.0, 2.0] # The list of aspect ratios for the anchor boxes\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "# 2: Optional: Load some weights\n",
    "\n",
    "#model.load_weights('./ssd7_weights.h5', by_name=True)\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t   800\n",
      "Number of images in the validation dataset:\t   150\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# 4: Define the image processing chain.\n",
    "\n",
    "# For the training generator:\n",
    "data_augmentation_chain = DataAugmentationVariableInputSize(resize_height=img_height,\n",
    "                                            resize_width=img_width)\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[\n",
    "                                                      resize],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ismail-P51\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the weights.\n",
    "model_checkpoint = ModelCheckpoint(filepath='ssd7_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd7_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - ETA: 58:40 - loss: 42.812 - ETA: 30:51 - loss: 32.002 - ETA: 27:59 - loss: 27.469 - ETA: 26:41 - loss: 24.984 - ETA: 22:02 - loss: 23.305 - ETA: 18:53 - loss: 22.202 - ETA: 16:36 - loss: 21.018 - ETA: 14:57 - loss: 20.125 - ETA: 13:37 - loss: 19.389 - ETA: 12:32 - loss: 18.595 - ETA: 13:36 - loss: 17.981 - ETA: 12:45 - loss: 17.410 - ETA: 12:47 - loss: 16.857 - ETA: 11:59 - loss: 16.386 - ETA: 11:22 - loss: 15.900 - ETA: 10:43 - loss: 15.469 - ETA: 10:08 - loss: 15.054 - ETA: 9:46 - loss: 14.691 - ETA: 9:25 - loss: 14.30 - ETA: 9:06 - loss: 13.98 - ETA: 8:43 - loss: 13.66 - ETA: 8:25 - loss: 13.36 - ETA: 8:10 - loss: 13.08 - ETA: 7:51 - loss: 12.82 - ETA: 7:33 - loss: 12.59 - ETA: 7:17 - loss: 12.34 - ETA: 7:04 - loss: 12.13 - ETA: 6:51 - loss: 11.90 - ETA: 6:41 - loss: 11.69 - ETA: 6:34 - loss: 11.51 - ETA: 6:26 - loss: 11.34 - ETA: 6:18 - loss: 11.16 - ETA: 7:08 - loss: 10.98 - ETA: 7:00 - loss: 10.82 - ETA: 6:52 - loss: 10.67 - ETA: 6:41 - loss: 10.53 - ETA: 6:34 - loss: 10.38 - ETA: 6:29 - loss: 10.25 - ETA: 6:21 - loss: 10.13 - ETA: 6:12 - loss: 9.9981 - ETA: 6:07 - loss: 9.866 - ETA: 6:02 - loss: 9.753 - ETA: 6:03 - loss: 9.632 - ETA: 6:02 - loss: 9.516 - ETA: 6:00 - loss: 9.409 - ETA: 5:58 - loss: 9.311 - ETA: 5:55 - loss: 9.208 - ETA: 5:52 - loss: 9.120 - ETA: 5:47 - loss: 9.029 - ETA: 5:48 - loss: 8.944 - ETA: 5:47 - loss: 8.859 - ETA: 5:41 - loss: 8.786 - ETA: 5:40 - loss: 8.700 - ETA: 5:37 - loss: 8.621 - ETA: 5:34 - loss: 8.552 - ETA: 5:31 - loss: 8.479 - ETA: 5:25 - loss: 8.412 - ETA: 5:23 - loss: 8.347 - ETA: 5:23 - loss: 8.280 - ETA: 5:22 - loss: 8.216 - ETA: 5:20 - loss: 8.158 - ETA: 5:19 - loss: 8.099 - ETA: 5:15 - loss: 8.041 - ETA: 5:17 - loss: 7.983 - ETA: 5:15 - loss: 7.927 - ETA: 5:11 - loss: 7.869 - ETA: 5:10 - loss: 7.811 - ETA: 5:07 - loss: 7.758 - ETA: 5:06 - loss: 7.708 - ETA: 5:04 - loss: 7.659 - ETA: 5:01 - loss: 7.612 - ETA: 5:02 - loss: 7.563 - ETA: 4:59 - loss: 7.513 - ETA: 4:57 - loss: 7.466 - ETA: 4:57 - loss: 7.422 - ETA: 4:56 - loss: 7.378 - ETA: 4:53 - loss: 7.332 - ETA: 4:50 - loss: 7.292 - ETA: 4:49 - loss: 7.258 - ETA: 4:46 - loss: 7.216 - ETA: 4:44 - loss: 7.178 - ETA: 4:43 - loss: 7.141 - ETA: 4:44 - loss: 7.103 - ETA: 4:41 - loss: 7.065 - ETA: 4:40 - loss: 7.028 - ETA: 4:37 - loss: 6.990 - ETA: 4:36 - loss: 6.955 - ETA: 4:34 - loss: 6.920 - ETA: 4:34 - loss: 6.885 - ETA: 4:35 - loss: 6.850 - ETA: 4:33 - loss: 6.818 - ETA: 4:30 - loss: 6.783 - ETA: 4:29 - loss: 6.752 - ETA: 4:26 - loss: 6.724 - ETA: 4:24 - loss: 6.691 - ETA: 4:24 - loss: 6.664 - ETA: 4:22 - loss: 6.634 - ETA: 4:20 - loss: 6.608 - ETA: 4:19 - loss: 6.581 - ETA: 4:19 - loss: 6.552 - ETA: 4:16 - loss: 6.523 - ETA: 4:15 - loss: 6.496 - ETA: 4:12 - loss: 6.468 - ETA: 4:10 - loss: 6.440 - ETA: 4:09 - loss: 6.416 - ETA: 4:08 - loss: 6.390 - ETA: 4:07 - loss: 6.364 - ETA: 4:05 - loss: 6.339 - ETA: 4:05 - loss: 6.314 - ETA: 4:05 - loss: 6.290 - ETA: 4:04 - loss: 6.267 - ETA: 4:04 - loss: 6.243 - ETA: 4:03 - loss: 6.222 - ETA: 4:02 - loss: 6.199 - ETA: 4:00 - loss: 6.178 - ETA: 3:59 - loss: 6.157 - ETA: 3:59 - loss: 6.134 - ETA: 4:01 - loss: 6.113 - ETA: 4:00 - loss: 6.092 - ETA: 3:59 - loss: 6.074 - ETA: 3:58 - loss: 6.053 - ETA: 3:57 - loss: 6.037 - ETA: 3:56 - loss: 6.018 - ETA: 3:56 - loss: 5.997 - ETA: 3:55 - loss: 5.979 - ETA: 3:54 - loss: 5.960 - ETA: 3:52 - loss: 5.943 - ETA: 3:51 - loss: 5.927 - ETA: 3:50 - loss: 5.910 - ETA: 3:49 - loss: 5.891 - ETA: 3:48 - loss: 5.874 - ETA: 3:46 - loss: 5.859 - ETA: 3:45 - loss: 5.841 - ETA: 3:44 - loss: 5.824 - ETA: 3:43 - loss: 5.806 - ETA: 3:42 - loss: 5.789 - ETA: 3:40 - loss: 5.772 - ETA: 3:39 - loss: 5.755 - ETA: 3:39 - loss: 5.739 - ETA: 3:37 - loss: 5.727 - ETA: 3:36 - loss: 5.711 - ETA: 3:34 - loss: 5.696 - ETA: 3:32 - loss: 5.681 - ETA: 3:31 - loss: 5.666 - ETA: 3:29 - loss: 5.653 - ETA: 3:27 - loss: 5.638 - ETA: 3:25 - loss: 5.623 - ETA: 3:24 - loss: 5.609 - ETA: 3:22 - loss: 5.594 - ETA: 3:21 - loss: 5.581 - ETA: 3:20 - loss: 5.565 - ETA: 3:18 - loss: 5.552 - ETA: 3:17 - loss: 5.539 - ETA: 3:16 - loss: 5.527 - ETA: 3:15 - loss: 5.515 - ETA: 3:14 - loss: 5.501 - ETA: 3:13 - loss: 5.488 - ETA: 3:14 - loss: 5.476 - ETA: 3:12 - loss: 5.464 - ETA: 3:10 - loss: 5.452 - ETA: 3:10 - loss: 5.439 - ETA: 3:09 - loss: 5.430 - ETA: 3:07 - loss: 5.419 - ETA: 3:06 - loss: 5.407 - ETA: 3:05 - loss: 5.396 - ETA: 3:04 - loss: 5.384 - ETA: 3:03 - loss: 5.371 - ETA: 3:01 - loss: 5.358 - ETA: 3:00 - loss: 5.345 - ETA: 2:59 - loss: 5.334 - ETA: 2:59 - loss: 5.323 - ETA: 2:58 - loss: 5.312 - ETA: 2:56 - loss: 5.301 - ETA: 2:54 - loss: 5.288 - ETA: 2:52 - loss: 5.278 - ETA: 2:51 - loss: 5.266 - ETA: 2:49 - loss: 5.256 - ETA: 2:48 - loss: 5.245 - ETA: 2:46 - loss: 5.235 - ETA: 2:45 - loss: 5.225 - ETA: 2:43 - loss: 5.214 - ETA: 2:42 - loss: 5.204 - ETA: 2:40 - loss: 5.195 - ETA: 2:39 - loss: 5.185 - ETA: 2:37 - loss: 5.174 - ETA: 2:35 - loss: 5.163 - ETA: 2:34 - loss: 5.156 - ETA: 2:32 - loss: 5.146 - ETA: 2:31 - loss: 5.137 - ETA: 2:29 - loss: 5.128 - ETA: 2:27 - loss: 5.119 - ETA: 2:26 - loss: 5.109 - ETA: 2:24 - loss: 5.100 - ETA: 2:23 - loss: 5.092 - ETA: 2:21 - loss: 5.084 - ETA: 2:20 - loss: 5.075 - ETA: 2:19 - loss: 5.065 - ETA: 2:17 - loss: 5.056 - ETA: 2:16 - loss: 5.046 - ETA: 2:14 - loss: 5.037 - ETA: 2:13 - loss: 5.027 - ETA: 2:11 - loss: 5.020 - ETA: 2:10 - loss: 5.010 - ETA: 2:08 - loss: 5.001 - ETA: 2:07 - loss: 4.992 - ETA: 2:06 - loss: 4.983 - ETA: 2:04 - loss: 4.975 - ETA: 2:03 - loss: 4.966 - ETA: 2:01 - loss: 4.958 - ETA: 2:00 - loss: 4.949 - ETA: 1:58 - loss: 4.941 - ETA: 1:57 - loss: 4.933 - ETA: 1:56 - loss: 4.925 - ETA: 1:54 - loss: 4.917 - ETA: 1:53 - loss: 4.909 - ETA: 1:52 - loss: 4.901 - ETA: 1:51 - loss: 4.893 - ETA: 1:50 - loss: 4.886 - ETA: 1:49 - loss: 4.878 - ETA: 1:48 - loss: 4.869 - ETA: 1:46 - loss: 4.863 - ETA: 1:45 - loss: 4.855 - ETA: 1:44 - loss: 4.847 - ETA: 1:42 - loss: 4.840 - ETA: 1:41 - loss: 4.833 - ETA: 1:40 - loss: 4.826 - ETA: 1:39 - loss: 4.820 - ETA: 1:38 - loss: 4.812 - ETA: 1:36 - loss: 4.806 - ETA: 1:35 - loss: 4.800 - ETA: 1:34 - loss: 4.793 - ETA: 1:32 - loss: 4.786 - ETA: 1:31 - loss: 4.780 - ETA: 1:30 - loss: 4.774 - ETA: 1:29 - loss: 4.767 - ETA: 1:27 - loss: 4.761 - ETA: 1:26 - loss: 4.755 - ETA: 1:25 - loss: 4.749 - ETA: 1:24 - loss: 4.742 - ETA: 1:22 - loss: 4.735 - ETA: 1:21 - loss: 4.728 - ETA: 1:20 - loss: 4.722 - ETA: 1:19 - loss: 4.715 - ETA: 1:17 - loss: 4.708 - ETA: 1:16 - loss: 4.702 - ETA: 1:15 - loss: 4.695 - ETA: 1:13 - loss: 4.689 - ETA: 1:12 - loss: 4.683 - ETA: 1:10 - loss: 4.678 - ETA: 1:09 - loss: 4.671 - ETA: 1:08 - loss: 4.665 - ETA: 1:06 - loss: 4.659 - ETA: 1:05 - loss: 4.654 - ETA: 1:03 - loss: 4.649 - ETA: 1:02 - loss: 4.644 - ETA: 1:01 - loss: 4.638 - ETA: 59s - loss: 4.633 - ETA: 58s - loss: 4.62 - ETA: 57s - loss: 4.62 - ETA: 55s - loss: 4.61 - ETA: 54s - loss: 4.61 - ETA: 52s - loss: 4.60 - ETA: 51s - loss: 4.59 - ETA: 49s - loss: 4.59 - ETA: 48s - loss: 4.58 - ETA: 47s - loss: 4.58 - ETA: 45s - loss: 4.57 - ETA: 44s - loss: 4.57 - ETA: 43s - loss: 4.56 - ETA: 41s - loss: 4.56 - ETA: 40s - loss: 4.55 - ETA: 39s - loss: 4.55 - ETA: 37s - loss: 4.54 - ETA: 36s - loss: 4.54 - ETA: 35s - loss: 4.53 - ETA: 33s - loss: 4.53 - ETA: 32s - loss: 4.52 - ETA: 30s - loss: 4.52 - ETA: 29s - loss: 4.51 - ETA: 28s - loss: 4.51 - ETA: 26s - loss: 4.50 - ETA: 25s - loss: 4.50 - ETA: 23s - loss: 4.49 - ETA: 22s - loss: 4.49 - ETA: 21s - loss: 4.48 - ETA: 19s - loss: 4.48 - ETA: 18s - loss: 4.47 - ETA: 16s - loss: 4.47 - ETA: 15s - loss: 4.47 - ETA: 13s - loss: 4.46 - ETA: 12s - loss: 4.46 - ETA: 11s - loss: 4.45 - ETA: 9s - loss: 4.4527 - ETA: 8s - loss: 4.448 - ETA: 6s - loss: 4.443 - ETA: 5s - loss: 4.439 - ETA: 4s - loss: 4.435 - ETA: 2s - loss: 4.431 - ETA: 1s - loss: 4.427 - 424s 1s/step - loss: 4.4227 - val_loss: 3.2318\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.23184, saving model to ssd7_epoch-01_loss-4.4334_val_loss-3.2318.h5\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 2:32 - loss: 3.209 - ETA: 2:17 - loss: 3.132 - ETA: 2:05 - loss: 3.126 - ETA: 2:01 - loss: 3.123 - ETA: 2:02 - loss: 3.148 - ETA: 1:59 - loss: 3.149 - ETA: 1:59 - loss: 3.134 - ETA: 2:14 - loss: 3.152 - ETA: 2:48 - loss: 3.153 - ETA: 3:21 - loss: 3.138 - ETA: 3:31 - loss: 3.123 - ETA: 3:49 - loss: 3.117 - ETA: 3:55 - loss: 3.121 - ETA: 4:15 - loss: 3.113 - ETA: 4:39 - loss: 3.116 - ETA: 4:47 - loss: 3.118 - ETA: 4:57 - loss: 3.123 - ETA: 5:00 - loss: 3.114 - ETA: 5:01 - loss: 3.109 - ETA: 5:02 - loss: 3.103 - ETA: 5:07 - loss: 3.095 - ETA: 5:09 - loss: 3.093 - ETA: 5:09 - loss: 3.093 - ETA: 5:10 - loss: 3.094 - ETA: 5:13 - loss: 3.100 - ETA: 5:10 - loss: 3.103 - ETA: 5:08 - loss: 3.100 - ETA: 5:08 - loss: 3.102 - ETA: 5:13 - loss: 3.107 - ETA: 5:37 - loss: 3.110 - ETA: 5:38 - loss: 3.108 - ETA: 5:43 - loss: 3.108 - ETA: 5:42 - loss: 3.115 - ETA: 5:38 - loss: 3.115 - ETA: 5:41 - loss: 3.117 - ETA: 5:38 - loss: 3.115 - ETA: 5:42 - loss: 3.118 - ETA: 5:40 - loss: 3.119 - ETA: 5:40 - loss: 3.118 - ETA: 5:43 - loss: 3.114 - ETA: 5:41 - loss: 3.113 - ETA: 5:41 - loss: 3.113 - ETA: 5:41 - loss: 3.117 - ETA: 5:42 - loss: 3.122 - ETA: 5:40 - loss: 3.123 - ETA: 5:43 - loss: 3.126 - ETA: 5:44 - loss: 3.131 - ETA: 5:42 - loss: 3.128 - ETA: 5:44 - loss: 3.129 - ETA: 5:43 - loss: 3.130 - ETA: 5:41 - loss: 3.128 - ETA: 5:37 - loss: 3.125 - ETA: 5:35 - loss: 3.125 - ETA: 5:33 - loss: 3.124 - ETA: 5:31 - loss: 3.123 - ETA: 5:28 - loss: 3.119 - ETA: 5:26 - loss: 3.117 - ETA: 5:24 - loss: 3.114 - ETA: 5:22 - loss: 3.114 - ETA: 5:20 - loss: 3.113 - ETA: 5:18 - loss: 3.111 - ETA: 5:14 - loss: 3.108 - ETA: 5:13 - loss: 3.109 - ETA: 5:12 - loss: 3.106 - ETA: 5:11 - loss: 3.106 - ETA: 5:08 - loss: 3.104 - ETA: 5:07 - loss: 3.104 - ETA: 5:06 - loss: 3.103 - ETA: 5:09 - loss: 3.101 - ETA: 5:05 - loss: 3.101 - ETA: 5:09 - loss: 3.101 - ETA: 5:08 - loss: 3.102 - ETA: 5:05 - loss: 3.102 - ETA: 5:04 - loss: 3.102 - ETA: 5:01 - loss: 3.102 - ETA: 5:00 - loss: 3.102 - ETA: 4:57 - loss: 3.098 - ETA: 4:56 - loss: 3.097 - ETA: 4:56 - loss: 3.097 - ETA: 4:56 - loss: 3.097 - ETA: 4:57 - loss: 3.098 - ETA: 4:59 - loss: 3.099 - ETA: 5:00 - loss: 3.102 - ETA: 5:01 - loss: 3.102 - ETA: 5:01 - loss: 3.103 - ETA: 5:02 - loss: 3.103 - ETA: 5:00 - loss: 3.103 - ETA: 4:58 - loss: 3.103 - ETA: 4:58 - loss: 3.103 - ETA: 4:59 - loss: 3.103 - ETA: 4:56 - loss: 3.102 - ETA: 4:56 - loss: 3.103 - ETA: 4:55 - loss: 3.101 - ETA: 4:55 - loss: 3.101 - ETA: 4:52 - loss: 3.101 - ETA: 4:51 - loss: 3.100 - ETA: 4:49 - loss: 3.101 - ETA: 4:48 - loss: 3.102 - ETA: 4:46 - loss: 3.101 - ETA: 4:45 - loss: 3.100 - ETA: 4:43 - loss: 3.099 - ETA: 4:42 - loss: 3.098 - ETA: 4:41 - loss: 3.099 - ETA: 4:40 - loss: 3.099 - ETA: 4:39 - loss: 3.097 - ETA: 4:38 - loss: 3.095 - ETA: 4:39 - loss: 3.096 - ETA: 4:37 - loss: 3.096 - ETA: 4:36 - loss: 3.095 - ETA: 4:36 - loss: 3.097 - ETA: 4:34 - loss: 3.095 - ETA: 4:34 - loss: 3.096 - ETA: 4:34 - loss: 3.096 - ETA: 4:32 - loss: 3.094 - ETA: 4:31 - loss: 3.094 - ETA: 4:29 - loss: 3.094 - ETA: 4:27 - loss: 3.094 - ETA: 4:26 - loss: 3.093 - ETA: 4:26 - loss: 3.092 - ETA: 4:25 - loss: 3.091 - ETA: 4:23 - loss: 3.090 - ETA: 4:22 - loss: 3.090 - ETA: 4:21 - loss: 3.089 - ETA: 4:20 - loss: 3.088 - ETA: 4:21 - loss: 3.086 - ETA: 4:19 - loss: 3.086 - ETA: 4:18 - loss: 3.085 - ETA: 4:16 - loss: 3.085 - ETA: 4:17 - loss: 3.085 - ETA: 4:16 - loss: 3.083 - ETA: 4:15 - loss: 3.082 - ETA: 4:14 - loss: 3.081 - ETA: 4:12 - loss: 3.080 - ETA: 4:10 - loss: 3.080 - ETA: 4:10 - loss: 3.079 - ETA: 4:08 - loss: 3.077 - ETA: 4:05 - loss: 3.077 - ETA: 4:05 - loss: 3.075 - ETA: 4:03 - loss: 3.074 - ETA: 4:02 - loss: 3.074 - ETA: 4:01 - loss: 3.073 - ETA: 3:59 - loss: 3.072 - ETA: 3:58 - loss: 3.071 - ETA: 3:56 - loss: 3.069 - ETA: 3:54 - loss: 3.070 - ETA: 3:53 - loss: 3.069 - ETA: 3:51 - loss: 3.068 - ETA: 3:49 - loss: 3.067 - ETA: 3:47 - loss: 3.067 - ETA: 3:45 - loss: 3.067 - ETA: 3:43 - loss: 3.068 - ETA: 3:42 - loss: 3.069 - ETA: 3:40 - loss: 3.070 - ETA: 3:38 - loss: 3.069 - ETA: 3:37 - loss: 3.069 - ETA: 3:35 - loss: 3.069 - ETA: 3:33 - loss: 3.069 - ETA: 3:32 - loss: 3.069 - ETA: 3:30 - loss: 3.069 - ETA: 3:28 - loss: 3.070 - ETA: 3:26 - loss: 3.069 - ETA: 3:24 - loss: 3.069 - ETA: 3:22 - loss: 3.068 - ETA: 3:20 - loss: 3.068 - ETA: 3:18 - loss: 3.067 - ETA: 3:17 - loss: 3.066 - ETA: 3:14 - loss: 3.066 - ETA: 3:13 - loss: 3.065 - ETA: 3:11 - loss: 3.064 - ETA: 3:10 - loss: 3.063 - ETA: 3:08 - loss: 3.062 - ETA: 3:06 - loss: 3.061 - ETA: 3:05 - loss: 3.060 - ETA: 3:04 - loss: 3.060 - ETA: 3:02 - loss: 3.059 - ETA: 3:01 - loss: 3.059 - ETA: 2:59 - loss: 3.058 - ETA: 2:58 - loss: 3.057 - ETA: 2:56 - loss: 3.056 - ETA: 2:54 - loss: 3.056 - ETA: 2:53 - loss: 3.055 - ETA: 2:51 - loss: 3.055 - ETA: 2:49 - loss: 3.054 - ETA: 2:48 - loss: 3.053 - ETA: 2:45 - loss: 3.052 - ETA: 2:45 - loss: 3.051 - ETA: 2:43 - loss: 3.050 - ETA: 2:41 - loss: 3.050 - ETA: 2:40 - loss: 3.049 - ETA: 2:38 - loss: 3.048 - ETA: 2:37 - loss: 3.047 - ETA: 2:35 - loss: 3.046 - ETA: 2:34 - loss: 3.046 - ETA: 2:32 - loss: 3.045 - ETA: 2:30 - loss: 3.044 - ETA: 2:29 - loss: 3.044 - ETA: 2:27 - loss: 3.043 - ETA: 2:26 - loss: 3.042 - ETA: 2:24 - loss: 3.041 - ETA: 2:23 - loss: 3.041 - ETA: 2:21 - loss: 3.041 - ETA: 2:20 - loss: 3.040 - ETA: 2:18 - loss: 3.040 - ETA: 2:17 - loss: 3.040 - ETA: 2:15 - loss: 3.039 - ETA: 2:14 - loss: 3.038 - ETA: 2:12 - loss: 3.038 - ETA: 2:11 - loss: 3.037 - ETA: 2:09 - loss: 3.036 - ETA: 2:08 - loss: 3.036 - ETA: 2:06 - loss: 3.035 - ETA: 2:05 - loss: 3.034 - ETA: 2:03 - loss: 3.035 - ETA: 2:02 - loss: 3.034 - ETA: 2:00 - loss: 3.034 - ETA: 1:59 - loss: 3.033 - ETA: 1:58 - loss: 3.032 - ETA: 1:56 - loss: 3.033 - ETA: 1:54 - loss: 3.033 - ETA: 1:53 - loss: 3.032 - ETA: 1:51 - loss: 3.031 - ETA: 1:50 - loss: 3.030 - ETA: 1:49 - loss: 3.030 - ETA: 1:47 - loss: 3.030 - ETA: 1:46 - loss: 3.029 - ETA: 1:44 - loss: 3.029 - ETA: 1:43 - loss: 3.028 - ETA: 1:41 - loss: 3.028 - ETA: 1:40 - loss: 3.028 - ETA: 1:38 - loss: 3.027 - ETA: 1:37 - loss: 3.027 - ETA: 1:36 - loss: 3.027 - ETA: 1:34 - loss: 3.026 - ETA: 1:33 - loss: 3.026 - ETA: 1:31 - loss: 3.026 - ETA: 1:30 - loss: 3.025 - ETA: 1:29 - loss: 3.024 - ETA: 1:27 - loss: 3.024 - ETA: 1:26 - loss: 3.023 - ETA: 1:24 - loss: 3.023 - ETA: 1:23 - loss: 3.022 - ETA: 1:21 - loss: 3.022 - ETA: 1:20 - loss: 3.022 - ETA: 1:19 - loss: 3.021 - ETA: 1:17 - loss: 3.021 - ETA: 1:16 - loss: 3.021 - ETA: 1:14 - loss: 3.021 - ETA: 1:13 - loss: 3.020 - ETA: 1:11 - loss: 3.020 - ETA: 1:10 - loss: 3.019 - ETA: 1:08 - loss: 3.019 - ETA: 1:07 - loss: 3.019 - ETA: 1:05 - loss: 3.018 - ETA: 1:04 - loss: 3.018 - ETA: 1:03 - loss: 3.018 - ETA: 1:01 - loss: 3.017 - ETA: 1:00 - loss: 3.017 - ETA: 58s - loss: 3.017 - ETA: 57s - loss: 3.01 - ETA: 55s - loss: 3.01 - ETA: 54s - loss: 3.01 - ETA: 52s - loss: 3.01 - ETA: 51s - loss: 3.01 - ETA: 50s - loss: 3.01 - ETA: 48s - loss: 3.01 - ETA: 47s - loss: 3.01 - ETA: 45s - loss: 3.01 - ETA: 44s - loss: 3.01 - ETA: 43s - loss: 3.01 - ETA: 41s - loss: 3.01 - ETA: 40s - loss: 3.01 - ETA: 38s - loss: 3.01 - ETA: 37s - loss: 3.01 - ETA: 36s - loss: 3.01 - ETA: 34s - loss: 3.01 - ETA: 33s - loss: 3.01 - ETA: 31s - loss: 3.01 - ETA: 30s - loss: 3.01 - ETA: 29s - loss: 3.01 - ETA: 27s - loss: 3.01 - ETA: 26s - loss: 3.01 - ETA: 24s - loss: 3.00 - ETA: 23s - loss: 3.00 - ETA: 22s - loss: 3.00 - ETA: 20s - loss: 3.00 - ETA: 19s - loss: 3.00 - ETA: 17s - loss: 3.00 - ETA: 16s - loss: 3.00 - ETA: 15s - loss: 3.00 - ETA: 13s - loss: 3.00 - ETA: 12s - loss: 3.00 - ETA: 11s - loss: 3.00 - ETA: 9s - loss: 3.0048 - ETA: 8s - loss: 3.004 - ETA: 6s - loss: 3.004 - ETA: 5s - loss: 3.003 - ETA: 4s - loss: 3.003 - ETA: 2s - loss: 3.003 - ETA: 1s - loss: 3.003 - 421s 1s/step - loss: 3.0029 - val_loss: 2.8939\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.23184 to 2.89392, saving model to ssd7_epoch-02_loss-3.0043_val_loss-2.8939.h5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 3:27 - loss: 2.821 - ETA: 3:59 - loss: 2.827 - ETA: 3:50 - loss: 2.861 - ETA: 4:07 - loss: 2.907 - ETA: 4:17 - loss: 2.909 - ETA: 4:19 - loss: 2.902 - ETA: 4:36 - loss: 2.929 - ETA: 4:49 - loss: 2.937 - ETA: 5:11 - loss: 2.936 - ETA: 5:00 - loss: 2.938 - ETA: 5:26 - loss: 2.936 - ETA: 5:27 - loss: 2.934 - ETA: 5:44 - loss: 2.961 - ETA: 5:53 - loss: 2.965 - ETA: 5:59 - loss: 2.955 - ETA: 6:05 - loss: 2.948 - ETA: 6:05 - loss: 2.943 - ETA: 6:02 - loss: 2.942 - ETA: 6:02 - loss: 2.934 - ETA: 6:33 - loss: 2.939 - ETA: 6:35 - loss: 2.937 - ETA: 6:29 - loss: 2.941 - ETA: 6:36 - loss: 2.941 - ETA: 6:31 - loss: 2.936 - ETA: 6:26 - loss: 2.932 - ETA: 6:20 - loss: 2.934 - ETA: 6:16 - loss: 2.937 - ETA: 6:16 - loss: 2.935 - ETA: 6:18 - loss: 2.931 - ETA: 6:13 - loss: 2.927 - ETA: 6:10 - loss: 2.925 - ETA: 6:06 - loss: 2.929 - ETA: 6:05 - loss: 2.928 - ETA: 5:59 - loss: 2.925 - ETA: 5:59 - loss: 2.924 - ETA: 6:02 - loss: 2.924 - ETA: 6:01 - loss: 2.921 - ETA: 5:54 - loss: 2.920 - ETA: 5:50 - loss: 2.921 - ETA: 5:49 - loss: 2.918 - ETA: 5:48 - loss: 2.917 - ETA: 5:47 - loss: 2.916 - ETA: 5:44 - loss: 2.915 - ETA: 5:39 - loss: 2.915 - ETA: 5:39 - loss: 2.921 - ETA: 5:37 - loss: 2.920 - ETA: 5:37 - loss: 2.922 - ETA: 5:34 - loss: 2.922 - ETA: 5:34 - loss: 2.926 - ETA: 5:31 - loss: 2.926 - ETA: 5:29 - loss: 2.924 - ETA: 5:27 - loss: 2.923 - ETA: 5:28 - loss: 2.922 - ETA: 5:27 - loss: 2.922 - ETA: 5:26 - loss: 2.920 - ETA: 5:24 - loss: 2.919 - ETA: 5:19 - loss: 2.923 - ETA: 5:18 - loss: 2.923 - ETA: 5:16 - loss: 2.923 - ETA: 5:12 - loss: 2.922 - ETA: 5:11 - loss: 2.923 - ETA: 5:10 - loss: 2.922 - ETA: 5:07 - loss: 2.924 - ETA: 5:04 - loss: 2.922 - ETA: 5:03 - loss: 2.922 - ETA: 5:01 - loss: 2.921 - ETA: 4:58 - loss: 2.920 - ETA: 4:56 - loss: 2.921 - ETA: 4:54 - loss: 2.921 - ETA: 4:55 - loss: 2.920 - ETA: 4:54 - loss: 2.920 - ETA: 4:52 - loss: 2.921 - ETA: 4:52 - loss: 2.919 - ETA: 4:51 - loss: 2.920 - ETA: 4:49 - loss: 2.919 - ETA: 4:48 - loss: 2.918 - ETA: 4:46 - loss: 2.918 - ETA: 4:42 - loss: 2.917 - ETA: 4:42 - loss: 2.915 - ETA: 4:39 - loss: 2.917 - ETA: 4:38 - loss: 2.918 - ETA: 4:39 - loss: 2.918 - ETA: 4:38 - loss: 2.916 - ETA: 4:37 - loss: 2.917 - ETA: 4:36 - loss: 2.918 - ETA: 4:34 - loss: 2.918 - ETA: 4:31 - loss: 2.918 - ETA: 4:32 - loss: 2.917 - ETA: 4:31 - loss: 2.916 - ETA: 4:29 - loss: 2.915 - ETA: 4:31 - loss: 2.914 - ETA: 4:30 - loss: 2.915 - ETA: 4:29 - loss: 2.917 - ETA: 4:29 - loss: 2.916 - ETA: 4:29 - loss: 2.915 - ETA: 4:27 - loss: 2.915 - ETA: 4:27 - loss: 2.915 - ETA: 4:27 - loss: 2.914 - ETA: 4:25 - loss: 2.916 - ETA: 4:23 - loss: 2.918 - ETA: 4:22 - loss: 2.919 - ETA: 4:20 - loss: 2.919 - ETA: 4:20 - loss: 2.918 - ETA: 4:19 - loss: 2.920 - ETA: 4:19 - loss: 2.921 - ETA: 4:18 - loss: 2.921 - ETA: 4:16 - loss: 2.920 - ETA: 4:14 - loss: 2.922 - ETA: 4:15 - loss: 2.922 - ETA: 4:14 - loss: 2.922 - ETA: 4:12 - loss: 2.921 - ETA: 4:11 - loss: 2.922 - ETA: 4:09 - loss: 2.921 - ETA: 4:08 - loss: 2.920 - ETA: 4:09 - loss: 2.920 - ETA: 4:07 - loss: 2.919 - ETA: 4:06 - loss: 2.920 - ETA: 4:05 - loss: 2.919 - ETA: 4:04 - loss: 2.921 - ETA: 4:03 - loss: 2.922 - ETA: 4:00 - loss: 2.922 - ETA: 3:59 - loss: 2.922 - ETA: 3:58 - loss: 2.922 - ETA: 3:56 - loss: 2.922 - ETA: 3:54 - loss: 2.922 - ETA: 3:53 - loss: 2.922 - ETA: 3:52 - loss: 2.922 - ETA: 3:50 - loss: 2.922 - ETA: 3:49 - loss: 2.921 - ETA: 3:47 - loss: 2.920 - ETA: 3:45 - loss: 2.920 - ETA: 3:44 - loss: 2.920 - ETA: 3:41 - loss: 2.920 - ETA: 3:40 - loss: 2.919 - ETA: 3:39 - loss: 2.919 - ETA: 3:37 - loss: 2.918 - ETA: 3:35 - loss: 2.918 - ETA: 3:33 - loss: 2.919 - ETA: 3:33 - loss: 2.919 - ETA: 3:31 - loss: 2.919 - ETA: 3:30 - loss: 2.919 - ETA: 3:28 - loss: 2.918 - ETA: 3:27 - loss: 2.917 - ETA: 3:25 - loss: 2.917 - ETA: 3:23 - loss: 2.916 - ETA: 3:23 - loss: 2.915 - ETA: 3:21 - loss: 2.916 - ETA: 3:20 - loss: 2.916 - ETA: 3:18 - loss: 2.916 - ETA: 3:17 - loss: 2.918 - ETA: 3:15 - loss: 2.918 - ETA: 3:14 - loss: 2.919 - ETA: 3:12 - loss: 2.918 - ETA: 3:11 - loss: 2.918 - ETA: 3:10 - loss: 2.918 - ETA: 3:08 - loss: 2.918 - ETA: 3:06 - loss: 2.917 - ETA: 3:05 - loss: 2.916 - ETA: 3:03 - loss: 2.915 - ETA: 3:02 - loss: 2.916 - ETA: 3:00 - loss: 2.915 - ETA: 3:00 - loss: 2.915 - ETA: 2:58 - loss: 2.914 - ETA: 2:57 - loss: 2.914 - ETA: 2:55 - loss: 2.914 - ETA: 2:54 - loss: 2.914 - ETA: 2:52 - loss: 2.913 - ETA: 2:51 - loss: 2.913 - ETA: 2:49 - loss: 2.913 - ETA: 2:48 - loss: 2.912 - ETA: 2:47 - loss: 2.912 - ETA: 2:45 - loss: 2.912 - ETA: 2:44 - loss: 2.911 - ETA: 2:43 - loss: 2.911 - ETA: 2:42 - loss: 2.910 - ETA: 2:41 - loss: 2.909 - ETA: 2:40 - loss: 2.909 - ETA: 2:39 - loss: 2.909 - ETA: 2:39 - loss: 2.909 - ETA: 2:37 - loss: 2.909 - ETA: 2:36 - loss: 2.909 - ETA: 2:35 - loss: 2.909 - ETA: 2:34 - loss: 2.910 - ETA: 2:33 - loss: 2.910 - ETA: 2:32 - loss: 2.909 - ETA: 2:31 - loss: 2.909 - ETA: 2:29 - loss: 2.910 - ETA: 2:28 - loss: 2.911 - ETA: 2:27 - loss: 2.910 - ETA: 2:26 - loss: 2.911 - ETA: 2:25 - loss: 2.911 - ETA: 2:23 - loss: 2.911 - ETA: 2:22 - loss: 2.911 - ETA: 2:21 - loss: 2.911 - ETA: 2:20 - loss: 2.911 - ETA: 2:18 - loss: 2.910 - ETA: 2:17 - loss: 2.912 - ETA: 2:16 - loss: 2.912 - ETA: 2:14 - loss: 2.912 - ETA: 2:13 - loss: 2.912 - ETA: 2:12 - loss: 2.912 - ETA: 2:10 - loss: 2.912 - ETA: 2:09 - loss: 2.913 - ETA: 2:08 - loss: 2.913 - ETA: 2:07 - loss: 2.913 - ETA: 2:06 - loss: 2.913 - ETA: 2:04 - loss: 2.913 - ETA: 2:03 - loss: 2.913 - ETA: 2:02 - loss: 2.913 - ETA: 2:01 - loss: 2.913 - ETA: 2:00 - loss: 2.912 - ETA: 1:59 - loss: 2.912 - ETA: 1:57 - loss: 2.912 - ETA: 1:56 - loss: 2.912 - ETA: 1:54 - loss: 2.912 - ETA: 1:53 - loss: 2.912 - ETA: 1:51 - loss: 2.912 - ETA: 1:50 - loss: 2.912 - ETA: 1:49 - loss: 2.913 - ETA: 1:48 - loss: 2.912 - ETA: 1:47 - loss: 2.912 - ETA: 1:46 - loss: 2.912 - ETA: 1:44 - loss: 2.912 - ETA: 1:43 - loss: 2.912 - ETA: 1:42 - loss: 2.912 - ETA: 1:40 - loss: 2.913 - ETA: 1:39 - loss: 2.913 - ETA: 1:37 - loss: 2.912 - ETA: 1:36 - loss: 2.912 - ETA: 1:34 - loss: 2.912 - ETA: 1:33 - loss: 2.911 - ETA: 1:31 - loss: 2.911 - ETA: 1:30 - loss: 2.911 - ETA: 1:29 - loss: 2.911 - ETA: 1:28 - loss: 2.911 - ETA: 1:26 - loss: 2.910 - ETA: 1:25 - loss: 2.910 - ETA: 1:24 - loss: 2.910 - ETA: 1:23 - loss: 2.911 - ETA: 1:21 - loss: 2.910 - ETA: 1:20 - loss: 2.910 - ETA: 1:19 - loss: 2.910 - ETA: 1:17 - loss: 2.910 - ETA: 1:16 - loss: 2.909 - ETA: 1:14 - loss: 2.909 - ETA: 1:13 - loss: 2.909 - ETA: 1:12 - loss: 2.910 - ETA: 1:11 - loss: 2.910 - ETA: 1:09 - loss: 2.909 - ETA: 1:08 - loss: 2.909 - ETA: 1:07 - loss: 2.909 - ETA: 1:06 - loss: 2.909 - ETA: 1:04 - loss: 2.908 - ETA: 1:03 - loss: 2.908 - ETA: 1:02 - loss: 2.908 - ETA: 1:01 - loss: 2.908 - ETA: 59s - loss: 2.907 - ETA: 58s - loss: 2.90 - ETA: 56s - loss: 2.90 - ETA: 55s - loss: 2.90 - ETA: 54s - loss: 2.90 - ETA: 52s - loss: 2.90 - ETA: 51s - loss: 2.90 - ETA: 49s - loss: 2.90 - ETA: 48s - loss: 2.90 - ETA: 47s - loss: 2.90 - ETA: 45s - loss: 2.90 - ETA: 44s - loss: 2.90 - ETA: 43s - loss: 2.90 - ETA: 41s - loss: 2.90 - ETA: 40s - loss: 2.90 - ETA: 38s - loss: 2.90 - ETA: 37s - loss: 2.90 - ETA: 36s - loss: 2.90 - ETA: 34s - loss: 2.90 - ETA: 33s - loss: 2.90 - ETA: 31s - loss: 2.90 - ETA: 30s - loss: 2.90 - ETA: 29s - loss: 2.90 - ETA: 27s - loss: 2.90 - ETA: 26s - loss: 2.90 - ETA: 25s - loss: 2.90 - ETA: 23s - loss: 2.90 - ETA: 22s - loss: 2.90 - ETA: 20s - loss: 2.90 - ETA: 19s - loss: 2.90 - ETA: 18s - loss: 2.90 - ETA: 16s - loss: 2.90 - ETA: 15s - loss: 2.90 - ETA: 13s - loss: 2.90 - ETA: 12s - loss: 2.90 - ETA: 11s - loss: 2.90 - ETA: 9s - loss: 2.9008 - ETA: 8s - loss: 2.901 - ETA: 6s - loss: 2.900 - ETA: 5s - loss: 2.900 - ETA: 4s - loss: 2.900 - ETA: 2s - loss: 2.900 - ETA: 1s - loss: 2.900 - 423s 1s/step - loss: 2.9001 - val_loss: 3.0110\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.89392\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 2:32 - loss: 2.834 - ETA: 3:18 - loss: 2.857 - ETA: 3:13 - loss: 2.863 - ETA: 3:13 - loss: 2.853 - ETA: 3:23 - loss: 2.872 - ETA: 3:33 - loss: 2.876 - ETA: 3:20 - loss: 2.884 - ETA: 3:25 - loss: 2.883 - ETA: 3:44 - loss: 2.893 - ETA: 4:30 - loss: 2.900 - ETA: 5:07 - loss: 2.907 - ETA: 5:17 - loss: 2.911 - ETA: 6:16 - loss: 2.908 - ETA: 6:23 - loss: 2.908 - ETA: 6:25 - loss: 2.915 - ETA: 6:37 - loss: 2.922 - ETA: 6:58 - loss: 2.921 - ETA: 6:59 - loss: 2.919 - ETA: 7:01 - loss: 2.918 - ETA: 7:05 - loss: 2.914 - ETA: 7:05 - loss: 2.919 - ETA: 6:58 - loss: 2.915 - ETA: 6:55 - loss: 2.913 - ETA: 6:52 - loss: 2.912 - ETA: 6:46 - loss: 2.908 - ETA: 6:44 - loss: 2.905 - ETA: 6:38 - loss: 2.906 - ETA: 6:40 - loss: 2.906 - ETA: 6:37 - loss: 2.904 - ETA: 6:39 - loss: 2.911 - ETA: 6:38 - loss: 2.908 - ETA: 6:34 - loss: 2.908 - ETA: 6:35 - loss: 2.907 - ETA: 6:29 - loss: 2.904 - ETA: 6:27 - loss: 2.900 - ETA: 6:29 - loss: 2.901 - ETA: 6:30 - loss: 2.901 - ETA: 6:27 - loss: 2.898 - ETA: 6:26 - loss: 2.899 - ETA: 6:35 - loss: 2.895 - ETA: 6:34 - loss: 2.894 - ETA: 6:29 - loss: 2.894 - ETA: 6:33 - loss: 2.894 - ETA: 6:36 - loss: 2.895 - ETA: 6:37 - loss: 2.894 - ETA: 6:34 - loss: 2.893 - ETA: 6:32 - loss: 2.893 - ETA: 6:25 - loss: 2.891 - ETA: 6:24 - loss: 2.895 - ETA: 6:25 - loss: 2.894 - ETA: 6:17 - loss: 2.896 - ETA: 6:19 - loss: 2.896 - ETA: 6:17 - loss: 2.896 - ETA: 6:17 - loss: 2.894 - ETA: 6:15 - loss: 2.892 - ETA: 6:13 - loss: 2.891 - ETA: 6:10 - loss: 2.894 - ETA: 6:21 - loss: 2.892 - ETA: 6:19 - loss: 2.890 - ETA: 6:17 - loss: 2.889 - ETA: 6:13 - loss: 2.890 - ETA: 6:13 - loss: 2.888 - ETA: 6:11 - loss: 2.886 - ETA: 6:09 - loss: 2.885 - ETA: 6:06 - loss: 2.883 - ETA: 6:07 - loss: 2.881 - ETA: 6:05 - loss: 2.879 - ETA: 6:06 - loss: 2.878 - ETA: 6:03 - loss: 2.876 - ETA: 6:00 - loss: 2.875 - ETA: 5:56 - loss: 2.875 - ETA: 5:53 - loss: 2.875 - ETA: 5:51 - loss: 2.873 - ETA: 5:49 - loss: 2.871 - ETA: 5:47 - loss: 2.870 - ETA: 5:46 - loss: 2.869 - ETA: 5:42 - loss: 2.868 - ETA: 5:40 - loss: 2.867 - ETA: 5:37 - loss: 2.865 - ETA: 5:34 - loss: 2.866 - ETA: 5:31 - loss: 2.865 - ETA: 5:28 - loss: 2.866 - ETA: 5:27 - loss: 2.866 - ETA: 5:27 - loss: 2.867 - ETA: 5:25 - loss: 2.867 - ETA: 5:25 - loss: 2.869 - ETA: 5:22 - loss: 2.869 - ETA: 5:19 - loss: 2.868 - ETA: 5:16 - loss: 2.868 - ETA: 5:15 - loss: 2.869 - ETA: 5:12 - loss: 2.869 - ETA: 5:09 - loss: 2.868 - ETA: 5:07 - loss: 2.868 - ETA: 5:05 - loss: 2.867 - ETA: 5:05 - loss: 2.869 - ETA: 5:02 - loss: 2.868 - ETA: 5:00 - loss: 2.868 - ETA: 4:57 - loss: 2.868 - ETA: 4:55 - loss: 2.868 - ETA: 4:52 - loss: 2.870 - ETA: 4:51 - loss: 2.870 - ETA: 4:50 - loss: 2.870 - ETA: 4:48 - loss: 2.870 - ETA: 4:47 - loss: 2.870 - ETA: 4:44 - loss: 2.870 - ETA: 4:43 - loss: 2.870 - ETA: 4:40 - loss: 2.869 - ETA: 4:40 - loss: 2.869 - ETA: 4:38 - loss: 2.869 - ETA: 4:36 - loss: 2.869 - ETA: 4:34 - loss: 2.868 - ETA: 4:31 - loss: 2.868 - ETA: 4:29 - loss: 2.869 - ETA: 4:27 - loss: 2.868 - ETA: 4:25 - loss: 2.868 - ETA: 4:23 - loss: 2.867 - ETA: 4:22 - loss: 2.867 - ETA: 4:20 - loss: 2.866 - ETA: 4:18 - loss: 2.866 - ETA: 4:19 - loss: 2.866 - ETA: 4:18 - loss: 2.866 - ETA: 4:17 - loss: 2.866 - ETA: 4:17 - loss: 2.866 - ETA: 4:15 - loss: 2.866 - ETA: 4:14 - loss: 2.865 - ETA: 4:12 - loss: 2.865 - ETA: 4:10 - loss: 2.864 - ETA: 4:07 - loss: 2.862 - ETA: 4:06 - loss: 2.862 - ETA: 4:05 - loss: 2.862 - ETA: 4:03 - loss: 2.862 - ETA: 4:02 - loss: 2.863 - ETA: 4:01 - loss: 2.862 - ETA: 3:59 - loss: 2.862 - ETA: 3:59 - loss: 2.861 - ETA: 3:58 - loss: 2.862 - ETA: 3:57 - loss: 2.863 - ETA: 3:56 - loss: 2.865 - ETA: 3:54 - loss: 2.865 - ETA: 3:53 - loss: 2.867 - ETA: 3:52 - loss: 2.866 - ETA: 3:50 - loss: 2.866 - ETA: 3:48 - loss: 2.868 - ETA: 3:48 - loss: 2.870 - ETA: 3:47 - loss: 2.870 - ETA: 3:46 - loss: 2.869 - ETA: 3:44 - loss: 2.870 - ETA: 3:43 - loss: 2.870 - ETA: 3:42 - loss: 2.871 - ETA: 3:40 - loss: 2.872 - ETA: 3:39 - loss: 2.875 - ETA: 3:38 - loss: 2.874 - ETA: 3:37 - loss: 2.874 - ETA: 3:36 - loss: 2.875 - ETA: 3:35 - loss: 2.877 - ETA: 3:33 - loss: 2.877 - ETA: 3:31 - loss: 2.877 - ETA: 3:30 - loss: 2.876 - ETA: 3:28 - loss: 2.876 - ETA: 3:26 - loss: 2.876 - ETA: 3:25 - loss: 2.878 - ETA: 3:23 - loss: 2.879 - ETA: 3:21 - loss: 2.881 - ETA: 3:19 - loss: 2.880 - ETA: 3:18 - loss: 2.880 - ETA: 3:16 - loss: 2.880 - ETA: 3:15 - loss: 2.881 - ETA: 3:13 - loss: 2.882 - ETA: 3:11 - loss: 2.882 - ETA: 3:10 - loss: 2.882 - ETA: 3:08 - loss: 2.882 - ETA: 3:06 - loss: 2.882 - ETA: 3:04 - loss: 2.883 - ETA: 3:03 - loss: 2.882 - ETA: 3:01 - loss: 2.882 - ETA: 2:59 - loss: 2.882 - ETA: 2:58 - loss: 2.882 - ETA: 2:56 - loss: 2.882 - ETA: 2:55 - loss: 2.883 - ETA: 2:53 - loss: 2.882 - ETA: 2:52 - loss: 2.882 - ETA: 2:51 - loss: 2.882 - ETA: 2:49 - loss: 2.882 - ETA: 2:48 - loss: 2.882 - ETA: 2:46 - loss: 2.882 - ETA: 2:44 - loss: 2.881 - ETA: 2:42 - loss: 2.880 - ETA: 2:41 - loss: 2.880 - ETA: 2:39 - loss: 2.881 - ETA: 2:38 - loss: 2.880 - ETA: 2:36 - loss: 2.880 - ETA: 2:35 - loss: 2.880 - ETA: 2:33 - loss: 2.879 - ETA: 2:32 - loss: 2.879 - ETA: 2:30 - loss: 2.879 - ETA: 2:29 - loss: 2.879 - ETA: 2:27 - loss: 2.878 - ETA: 2:26 - loss: 2.879 - ETA: 2:24 - loss: 2.878 - ETA: 2:22 - loss: 2.878 - ETA: 2:21 - loss: 2.877 - ETA: 2:19 - loss: 2.877 - ETA: 2:18 - loss: 2.876 - ETA: 2:17 - loss: 2.876 - ETA: 2:15 - loss: 2.876 - ETA: 2:14 - loss: 2.875 - ETA: 2:13 - loss: 2.875 - ETA: 2:11 - loss: 2.874 - ETA: 2:10 - loss: 2.874 - ETA: 2:09 - loss: 2.874 - ETA: 2:07 - loss: 2.873 - ETA: 2:06 - loss: 2.873 - ETA: 2:04 - loss: 2.872 - ETA: 2:03 - loss: 2.872 - ETA: 2:02 - loss: 2.872 - ETA: 2:00 - loss: 2.872 - ETA: 1:59 - loss: 2.871 - ETA: 1:58 - loss: 2.871 - ETA: 1:57 - loss: 2.870 - ETA: 1:55 - loss: 2.870 - ETA: 1:54 - loss: 2.870 - ETA: 1:52 - loss: 2.869 - ETA: 1:51 - loss: 2.869 - ETA: 1:49 - loss: 2.869 - ETA: 1:48 - loss: 2.868 - ETA: 1:46 - loss: 2.869 - ETA: 1:44 - loss: 2.870 - ETA: 1:43 - loss: 2.869 - ETA: 1:41 - loss: 2.868 - ETA: 1:40 - loss: 2.868 - ETA: 1:39 - loss: 2.868 - ETA: 1:37 - loss: 2.868 - ETA: 1:36 - loss: 2.867 - ETA: 1:34 - loss: 2.866 - ETA: 1:33 - loss: 2.866 - ETA: 1:31 - loss: 2.865 - ETA: 1:30 - loss: 2.865 - ETA: 1:28 - loss: 2.864 - ETA: 1:27 - loss: 2.864 - ETA: 1:25 - loss: 2.864 - ETA: 1:24 - loss: 2.863 - ETA: 1:22 - loss: 2.863 - ETA: 1:21 - loss: 2.863 - ETA: 1:19 - loss: 2.862 - ETA: 1:18 - loss: 2.862 - ETA: 1:16 - loss: 2.861 - ETA: 1:15 - loss: 2.861 - ETA: 1:13 - loss: 2.861 - ETA: 1:12 - loss: 2.861 - ETA: 1:10 - loss: 2.860 - ETA: 1:09 - loss: 2.860 - ETA: 1:07 - loss: 2.860 - ETA: 1:06 - loss: 2.860 - ETA: 1:04 - loss: 2.860 - ETA: 1:03 - loss: 2.860 - ETA: 1:01 - loss: 2.860 - ETA: 1:00 - loss: 2.861 - ETA: 59s - loss: 2.860 - ETA: 57s - loss: 2.86 - ETA: 56s - loss: 2.86 - ETA: 54s - loss: 2.86 - ETA: 53s - loss: 2.86 - ETA: 51s - loss: 2.86 - ETA: 50s - loss: 2.86 - ETA: 49s - loss: 2.86 - ETA: 47s - loss: 2.85 - ETA: 46s - loss: 2.85 - ETA: 44s - loss: 2.85 - ETA: 43s - loss: 2.85 - ETA: 42s - loss: 2.85 - ETA: 40s - loss: 2.85 - ETA: 39s - loss: 2.85 - ETA: 38s - loss: 2.85 - ETA: 36s - loss: 2.85 - ETA: 35s - loss: 2.85 - ETA: 33s - loss: 2.85 - ETA: 32s - loss: 2.85 - ETA: 30s - loss: 2.85 - ETA: 29s - loss: 2.85 - ETA: 27s - loss: 2.85 - ETA: 26s - loss: 2.85 - ETA: 25s - loss: 2.85 - ETA: 23s - loss: 2.85 - ETA: 22s - loss: 2.85 - ETA: 20s - loss: 2.85 - ETA: 19s - loss: 2.85 - ETA: 18s - loss: 2.85 - ETA: 16s - loss: 2.85 - ETA: 15s - loss: 2.85 - ETA: 13s - loss: 2.85 - ETA: 12s - loss: 2.85 - ETA: 11s - loss: 2.85 - ETA: 9s - loss: 2.8516 - ETA: 8s - loss: 2.851 - ETA: 6s - loss: 2.851 - ETA: 5s - loss: 2.851 - ETA: 4s - loss: 2.850 - ETA: 2s - loss: 2.850 - ETA: 1s - loss: 2.849 - 423s 1s/step - loss: 2.8494 - val_loss: 2.7778\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.89392 to 2.77780, saving model to ssd7_epoch-04_loss-2.8478_val_loss-2.7778.h5\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 2:49 - loss: 2.722 - ETA: 3:04 - loss: 2.696 - ETA: 2:55 - loss: 2.712 - ETA: 3:01 - loss: 2.713 - ETA: 2:55 - loss: 2.714 - ETA: 3:31 - loss: 2.719 - ETA: 4:05 - loss: 2.723 - ETA: 4:21 - loss: 2.726 - ETA: 4:27 - loss: 2.723 - ETA: 5:06 - loss: 2.724 - ETA: 5:08 - loss: 2.723 - ETA: 5:19 - loss: 2.717 - ETA: 5:34 - loss: 2.721 - ETA: 5:39 - loss: 2.722 - ETA: 5:46 - loss: 2.722 - ETA: 5:40 - loss: 2.726 - ETA: 5:53 - loss: 2.733 - ETA: 5:46 - loss: 2.735 - ETA: 5:42 - loss: 2.735 - ETA: 5:37 - loss: 2.732 - ETA: 5:33 - loss: 2.732 - ETA: 5:36 - loss: 2.731 - ETA: 5:34 - loss: 2.731 - ETA: 5:36 - loss: 2.732 - ETA: 5:31 - loss: 2.734 - ETA: 5:29 - loss: 2.741 - ETA: 5:26 - loss: 2.740 - ETA: 5:23 - loss: 2.742 - ETA: 5:29 - loss: 2.741 - ETA: 5:33 - loss: 2.745 - ETA: 5:36 - loss: 2.745 - ETA: 5:34 - loss: 2.749 - ETA: 5:32 - loss: 2.748 - ETA: 5:29 - loss: 2.750 - ETA: 5:29 - loss: 2.752 - ETA: 5:29 - loss: 2.753 - ETA: 5:26 - loss: 2.755 - ETA: 5:23 - loss: 2.756 - ETA: 5:26 - loss: 2.757 - ETA: 5:25 - loss: 2.756 - ETA: 5:22 - loss: 2.757 - ETA: 5:22 - loss: 2.757 - ETA: 5:27 - loss: 2.757 - ETA: 5:24 - loss: 2.758 - ETA: 5:23 - loss: 2.761 - ETA: 5:19 - loss: 2.762 - ETA: 5:18 - loss: 2.765 - ETA: 5:13 - loss: 2.766 - ETA: 5:13 - loss: 2.766 - ETA: 5:10 - loss: 2.766 - ETA: 5:08 - loss: 2.768 - ETA: 5:09 - loss: 2.771 - ETA: 5:07 - loss: 2.769 - ETA: 5:06 - loss: 2.771 - ETA: 5:03 - loss: 2.772 - ETA: 5:02 - loss: 2.772 - ETA: 4:59 - loss: 2.775 - ETA: 4:58 - loss: 2.775 - ETA: 4:59 - loss: 2.775 - ETA: 4:56 - loss: 2.775 - ETA: 4:54 - loss: 2.777 - ETA: 4:53 - loss: 2.776 - ETA: 4:52 - loss: 2.777 - ETA: 4:50 - loss: 2.777 - ETA: 4:47 - loss: 2.778 - ETA: 4:46 - loss: 2.777 - ETA: 4:43 - loss: 2.777 - ETA: 4:41 - loss: 2.778 - ETA: 4:40 - loss: 2.778 - ETA: 4:38 - loss: 2.780 - ETA: 4:36 - loss: 2.780 - ETA: 4:34 - loss: 2.780 - ETA: 4:31 - loss: 2.781 - ETA: 4:31 - loss: 2.781 - ETA: 4:28 - loss: 2.784 - ETA: 4:28 - loss: 2.784 - ETA: 4:26 - loss: 2.785 - ETA: 4:24 - loss: 2.784 - ETA: 4:22 - loss: 2.784 - ETA: 4:21 - loss: 2.784 - ETA: 4:20 - loss: 2.784 - ETA: 4:18 - loss: 2.784 - ETA: 4:16 - loss: 2.783 - ETA: 4:17 - loss: 2.782 - ETA: 4:15 - loss: 2.782 - ETA: 4:15 - loss: 2.781 - ETA: 4:13 - loss: 2.780 - ETA: 4:11 - loss: 2.779 - ETA: 4:09 - loss: 2.781 - ETA: 4:07 - loss: 2.781 - ETA: 4:06 - loss: 2.781 - ETA: 4:05 - loss: 2.781 - ETA: 4:03 - loss: 2.780 - ETA: 4:02 - loss: 2.779 - ETA: 4:01 - loss: 2.778 - ETA: 4:00 - loss: 2.780 - ETA: 3:58 - loss: 2.782 - ETA: 3:59 - loss: 2.782 - ETA: 3:58 - loss: 2.781 - ETA: 3:56 - loss: 2.782 - ETA: 3:55 - loss: 2.782 - ETA: 3:53 - loss: 2.782 - ETA: 3:52 - loss: 2.784 - ETA: 3:51 - loss: 2.784 - ETA: 3:49 - loss: 2.785 - ETA: 3:49 - loss: 2.785 - ETA: 3:48 - loss: 2.785 - ETA: 3:46 - loss: 2.786 - ETA: 3:45 - loss: 2.787 - ETA: 3:45 - loss: 2.788 - ETA: 3:43 - loss: 2.787 - ETA: 3:42 - loss: 2.787 - ETA: 3:41 - loss: 2.789 - ETA: 3:40 - loss: 2.790 - ETA: 3:39 - loss: 2.790 - ETA: 3:38 - loss: 2.789 - ETA: 3:37 - loss: 2.789 - ETA: 3:36 - loss: 2.790 - ETA: 3:34 - loss: 2.790 - ETA: 3:33 - loss: 2.790 - ETA: 3:32 - loss: 2.791 - ETA: 3:30 - loss: 2.793 - ETA: 3:28 - loss: 2.793 - ETA: 3:27 - loss: 2.794 - ETA: 3:26 - loss: 2.794 - ETA: 3:24 - loss: 2.795 - ETA: 3:23 - loss: 2.795 - ETA: 3:22 - loss: 2.795 - ETA: 3:20 - loss: 2.795 - ETA: 3:19 - loss: 2.795 - ETA: 3:18 - loss: 2.795 - ETA: 3:17 - loss: 2.795 - ETA: 3:16 - loss: 2.795 - ETA: 3:14 - loss: 2.796 - ETA: 3:13 - loss: 2.797 - ETA: 3:12 - loss: 2.797 - ETA: 3:11 - loss: 2.798 - ETA: 3:10 - loss: 2.799 - ETA: 3:08 - loss: 2.798 - ETA: 3:07 - loss: 2.798 - ETA: 3:07 - loss: 2.798 - ETA: 3:06 - loss: 2.798 - ETA: 3:05 - loss: 2.798 - ETA: 3:04 - loss: 2.797 - ETA: 3:03 - loss: 2.797 - ETA: 3:02 - loss: 2.796 - ETA: 3:00 - loss: 2.796 - ETA: 2:59 - loss: 2.795 - ETA: 2:58 - loss: 2.796 - ETA: 2:57 - loss: 2.796 - ETA: 2:56 - loss: 2.795 - ETA: 2:55 - loss: 2.796 - ETA: 2:53 - loss: 2.795 - ETA: 2:52 - loss: 2.795 - ETA: 2:51 - loss: 2.795 - ETA: 2:50 - loss: 2.795 - ETA: 2:48 - loss: 2.794 - ETA: 2:47 - loss: 2.793 - ETA: 2:45 - loss: 2.793 - ETA: 2:44 - loss: 2.793 - ETA: 2:43 - loss: 2.793 - ETA: 2:42 - loss: 2.792 - ETA: 2:40 - loss: 2.792 - ETA: 2:39 - loss: 2.791 - ETA: 2:38 - loss: 2.792 - ETA: 2:37 - loss: 2.791 - ETA: 2:36 - loss: 2.791 - ETA: 2:35 - loss: 2.790 - ETA: 2:34 - loss: 2.790 - ETA: 2:33 - loss: 2.789 - ETA: 2:32 - loss: 2.789 - ETA: 2:31 - loss: 2.789 - ETA: 2:30 - loss: 2.789 - ETA: 2:29 - loss: 2.788 - ETA: 2:27 - loss: 2.788 - ETA: 2:26 - loss: 2.787 - ETA: 2:25 - loss: 2.787 - ETA: 2:23 - loss: 2.787 - ETA: 2:22 - loss: 2.787 - ETA: 2:21 - loss: 2.787 - ETA: 2:20 - loss: 2.787 - ETA: 2:19 - loss: 2.787 - ETA: 2:18 - loss: 2.787 - ETA: 2:17 - loss: 2.787 - ETA: 2:15 - loss: 2.787 - ETA: 2:14 - loss: 2.787 - ETA: 2:13 - loss: 2.786 - ETA: 2:12 - loss: 2.786 - ETA: 2:10 - loss: 2.786 - ETA: 2:09 - loss: 2.786 - ETA: 2:08 - loss: 2.785 - ETA: 2:07 - loss: 2.785 - ETA: 2:06 - loss: 2.786 - ETA: 2:04 - loss: 2.786 - ETA: 2:03 - loss: 2.786 - ETA: 2:02 - loss: 2.786 - ETA: 2:01 - loss: 2.786 - ETA: 1:59 - loss: 2.786 - ETA: 1:58 - loss: 2.786 - ETA: 1:57 - loss: 2.785 - ETA: 1:56 - loss: 2.785 - ETA: 1:54 - loss: 2.785 - ETA: 1:53 - loss: 2.784 - ETA: 1:52 - loss: 2.784 - ETA: 1:51 - loss: 2.784 - ETA: 1:50 - loss: 2.784 - ETA: 1:49 - loss: 2.784 - ETA: 1:48 - loss: 2.784 - ETA: 1:46 - loss: 2.783 - ETA: 1:45 - loss: 2.783 - ETA: 1:44 - loss: 2.783 - ETA: 1:43 - loss: 2.783 - ETA: 1:41 - loss: 2.783 - ETA: 1:40 - loss: 2.783 - ETA: 1:39 - loss: 2.783 - ETA: 1:38 - loss: 2.783 - ETA: 1:37 - loss: 2.782 - ETA: 1:36 - loss: 2.782 - ETA: 1:35 - loss: 2.782 - ETA: 1:33 - loss: 2.781 - ETA: 1:32 - loss: 2.781 - ETA: 1:31 - loss: 2.781 - ETA: 1:30 - loss: 2.781 - ETA: 1:29 - loss: 2.781 - ETA: 1:28 - loss: 2.780 - ETA: 1:26 - loss: 2.780 - ETA: 1:25 - loss: 2.780 - ETA: 1:24 - loss: 2.780 - ETA: 1:23 - loss: 2.779 - ETA: 1:22 - loss: 2.779 - ETA: 1:21 - loss: 2.779 - ETA: 1:19 - loss: 2.779 - ETA: 1:18 - loss: 2.779 - ETA: 1:17 - loss: 2.779 - ETA: 1:16 - loss: 2.779 - ETA: 1:15 - loss: 2.778 - ETA: 1:13 - loss: 2.778 - ETA: 1:12 - loss: 2.779 - ETA: 1:11 - loss: 2.778 - ETA: 1:10 - loss: 2.778 - ETA: 1:09 - loss: 2.778 - ETA: 1:08 - loss: 2.778 - ETA: 1:06 - loss: 2.777 - ETA: 1:05 - loss: 2.778 - ETA: 1:04 - loss: 2.778 - ETA: 1:03 - loss: 2.778 - ETA: 1:02 - loss: 2.778 - ETA: 1:01 - loss: 2.778 - ETA: 1:00 - loss: 2.778 - ETA: 58s - loss: 2.777 - ETA: 57s - loss: 2.77 - ETA: 56s - loss: 2.77 - ETA: 55s - loss: 2.77 - ETA: 54s - loss: 2.77 - ETA: 53s - loss: 2.77 - ETA: 51s - loss: 2.77 - ETA: 50s - loss: 2.77 - ETA: 49s - loss: 2.77 - ETA: 48s - loss: 2.77 - ETA: 47s - loss: 2.77 - ETA: 45s - loss: 2.77 - ETA: 44s - loss: 2.77 - ETA: 43s - loss: 2.77 - ETA: 42s - loss: 2.77 - ETA: 41s - loss: 2.77 - ETA: 40s - loss: 2.77 - ETA: 38s - loss: 2.77 - ETA: 37s - loss: 2.77 - ETA: 36s - loss: 2.77 - ETA: 35s - loss: 2.77 - ETA: 34s - loss: 2.77 - ETA: 32s - loss: 2.77 - ETA: 31s - loss: 2.77 - ETA: 30s - loss: 2.77 - ETA: 29s - loss: 2.77 - ETA: 28s - loss: 2.77 - ETA: 27s - loss: 2.77 - ETA: 26s - loss: 2.77 - ETA: 24s - loss: 2.77 - ETA: 23s - loss: 2.77 - ETA: 22s - loss: 2.77 - ETA: 21s - loss: 2.77 - ETA: 20s - loss: 2.77 - ETA: 18s - loss: 2.77 - ETA: 17s - loss: 2.77 - ETA: 16s - loss: 2.77 - ETA: 15s - loss: 2.77 - ETA: 14s - loss: 2.77 - ETA: 13s - loss: 2.77 - ETA: 11s - loss: 2.77 - ETA: 10s - loss: 2.77 - ETA: 9s - loss: 2.7726 - ETA: 8s - loss: 2.772 - ETA: 7s - loss: 2.772 - ETA: 5s - loss: 2.772 - ETA: 4s - loss: 2.772 - ETA: 3s - loss: 2.772 - ETA: 2s - loss: 2.772 - ETA: 1s - loss: 2.771 - 361s 1s/step - loss: 2.7717 - val_loss: 2.7208\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.77780 to 2.72076, saving model to ssd7_epoch-05_loss-2.7712_val_loss-2.7208.h5\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 2:47 - loss: 2.726 - ETA: 2:51 - loss: 2.710 - ETA: 2:44 - loss: 2.694 - ETA: 3:19 - loss: 2.754 - ETA: 3:12 - loss: 2.779 - ETA: 3:10 - loss: 2.757 - ETA: 3:14 - loss: 2.740 - ETA: 3:54 - loss: 2.743 - ETA: 4:13 - loss: 2.739 - ETA: 4:31 - loss: 2.750 - ETA: 4:54 - loss: 2.745 - ETA: 5:19 - loss: 2.740 - ETA: 5:15 - loss: 2.735 - ETA: 5:16 - loss: 2.740 - ETA: 5:28 - loss: 2.743 - ETA: 5:37 - loss: 2.737 - ETA: 5:38 - loss: 2.733 - ETA: 5:41 - loss: 2.731 - ETA: 5:49 - loss: 2.737 - ETA: 5:41 - loss: 2.732 - ETA: 5:40 - loss: 2.738 - ETA: 5:34 - loss: 2.738 - ETA: 5:33 - loss: 2.739 - ETA: 5:27 - loss: 2.739 - ETA: 5:26 - loss: 2.737 - ETA: 5:28 - loss: 2.737 - ETA: 5:24 - loss: 2.733 - ETA: 5:22 - loss: 2.733 - ETA: 5:18 - loss: 2.732 - ETA: 5:24 - loss: 2.731 - ETA: 5:17 - loss: 2.729 - ETA: 5:17 - loss: 2.729 - ETA: 5:14 - loss: 2.729 - ETA: 5:14 - loss: 2.727 - ETA: 5:14 - loss: 2.727 - ETA: 5:11 - loss: 2.732 - ETA: 5:10 - loss: 2.731 - ETA: 5:06 - loss: 2.731 - ETA: 5:06 - loss: 2.729 - ETA: 5:13 - loss: 2.734 - ETA: 5:11 - loss: 2.733 - ETA: 5:11 - loss: 2.732 - ETA: 5:07 - loss: 2.733 - ETA: 5:08 - loss: 2.733 - ETA: 5:07 - loss: 2.735 - ETA: 5:07 - loss: 2.736 - ETA: 5:04 - loss: 2.735 - ETA: 5:05 - loss: 2.735 - ETA: 5:02 - loss: 2.735 - ETA: 5:00 - loss: 2.735 - ETA: 5:09 - loss: 2.734 - ETA: 5:08 - loss: 2.735 - ETA: 5:10 - loss: 2.733 - ETA: 5:11 - loss: 2.733 - ETA: 5:10 - loss: 2.732 - ETA: 5:14 - loss: 2.733 - ETA: 5:11 - loss: 2.734 - ETA: 5:11 - loss: 2.733 - ETA: 5:07 - loss: 2.734 - ETA: 5:05 - loss: 2.734 - ETA: 5:03 - loss: 2.733 - ETA: 5:01 - loss: 2.732 - ETA: 4:58 - loss: 2.732 - ETA: 4:56 - loss: 2.733 - ETA: 4:54 - loss: 2.733 - ETA: 4:51 - loss: 2.732 - ETA: 4:49 - loss: 2.735 - ETA: 4:46 - loss: 2.736 - ETA: 4:45 - loss: 2.737 - ETA: 4:43 - loss: 2.737 - ETA: 4:41 - loss: 2.736 - ETA: 4:41 - loss: 2.737 - ETA: 4:40 - loss: 2.737 - ETA: 4:40 - loss: 2.738 - ETA: 4:38 - loss: 2.738 - ETA: 4:37 - loss: 2.738 - ETA: 4:36 - loss: 2.739 - ETA: 4:34 - loss: 2.739 - ETA: 4:32 - loss: 2.739 - ETA: 4:33 - loss: 2.739 - ETA: 4:30 - loss: 2.740 - ETA: 4:29 - loss: 2.741 - ETA: 4:27 - loss: 2.741 - ETA: 4:26 - loss: 2.742 - ETA: 4:26 - loss: 2.742 - ETA: 4:23 - loss: 2.742 - ETA: 4:24 - loss: 2.744 - ETA: 4:21 - loss: 2.744 - ETA: 4:20 - loss: 2.744 - ETA: 4:18 - loss: 2.747 - ETA: 4:16 - loss: 2.746 - ETA: 4:14 - loss: 2.747 - ETA: 4:13 - loss: 2.747 - ETA: 4:11 - loss: 2.750 - ETA: 4:09 - loss: 2.749 - ETA: 4:07 - loss: 2.749 - ETA: 4:08 - loss: 2.748 - ETA: 4:06 - loss: 2.749 - ETA: 4:04 - loss: 2.749 - ETA: 4:04 - loss: 2.749 - ETA: 4:02 - loss: 2.748 - ETA: 4:00 - loss: 2.747 - ETA: 3:59 - loss: 2.747 - ETA: 3:58 - loss: 2.746 - ETA: 3:57 - loss: 2.745 - ETA: 3:55 - loss: 2.745 - ETA: 3:55 - loss: 2.745 - ETA: 3:56 - loss: 2.744 - ETA: 3:53 - loss: 2.743 - ETA: 3:52 - loss: 2.743 - ETA: 3:50 - loss: 2.742 - ETA: 3:49 - loss: 2.743 - ETA: 3:48 - loss: 2.742 - ETA: 3:46 - loss: 2.742 - ETA: 3:47 - loss: 2.741 - ETA: 3:45 - loss: 2.741 - ETA: 3:44 - loss: 2.740 - ETA: 3:43 - loss: 2.739 - ETA: 3:43 - loss: 2.738 - ETA: 3:42 - loss: 2.737 - ETA: 3:41 - loss: 2.737 - ETA: 3:40 - loss: 2.737 - ETA: 3:39 - loss: 2.737 - ETA: 3:37 - loss: 2.736 - ETA: 3:35 - loss: 2.736 - ETA: 3:33 - loss: 2.736 - ETA: 3:32 - loss: 2.736 - ETA: 3:30 - loss: 2.738 - ETA: 3:29 - loss: 2.739 - ETA: 3:28 - loss: 2.740 - ETA: 3:27 - loss: 2.741 - ETA: 3:25 - loss: 2.741 - ETA: 3:23 - loss: 2.741 - ETA: 3:22 - loss: 2.742 - ETA: 3:22 - loss: 2.743 - ETA: 3:20 - loss: 2.743 - ETA: 3:19 - loss: 2.743 - ETA: 3:18 - loss: 2.743 - ETA: 3:17 - loss: 2.743 - ETA: 3:15 - loss: 2.743 - ETA: 3:14 - loss: 2.743 - ETA: 3:13 - loss: 2.744 - ETA: 3:12 - loss: 2.744 - ETA: 3:10 - loss: 2.744 - ETA: 3:09 - loss: 2.746 - ETA: 3:08 - loss: 2.746 - ETA: 3:06 - loss: 2.745 - ETA: 3:05 - loss: 2.745 - ETA: 3:04 - loss: 2.745 - ETA: 3:03 - loss: 2.745 - ETA: 3:01 - loss: 2.746 - ETA: 3:00 - loss: 2.746 - ETA: 3:00 - loss: 2.746 - ETA: 2:59 - loss: 2.747 - ETA: 2:59 - loss: 2.747 - ETA: 2:59 - loss: 2.747 - ETA: 2:58 - loss: 2.748 - ETA: 2:57 - loss: 2.748 - ETA: 2:55 - loss: 2.749 - ETA: 2:54 - loss: 2.748 - ETA: 2:53 - loss: 2.748 - ETA: 2:51 - loss: 2.748 - ETA: 2:50 - loss: 2.748 - ETA: 2:49 - loss: 2.748 - ETA: 2:48 - loss: 2.749 - ETA: 2:46 - loss: 2.749 - ETA: 2:45 - loss: 2.749 - ETA: 2:43 - loss: 2.749 - ETA: 2:42 - loss: 2.748 - ETA: 2:41 - loss: 2.748 - ETA: 2:40 - loss: 2.748 - ETA: 2:38 - loss: 2.749 - ETA: 2:37 - loss: 2.749 - ETA: 2:36 - loss: 2.750 - ETA: 2:35 - loss: 2.749 - ETA: 2:34 - loss: 2.749 - ETA: 2:33 - loss: 2.749 - ETA: 2:31 - loss: 2.749 - ETA: 2:30 - loss: 2.749 - ETA: 2:28 - loss: 2.749 - ETA: 2:27 - loss: 2.749 - ETA: 2:25 - loss: 2.749 - ETA: 2:24 - loss: 2.748 - ETA: 2:22 - loss: 2.747 - ETA: 2:21 - loss: 2.748 - ETA: 2:20 - loss: 2.748 - ETA: 2:18 - loss: 2.748 - ETA: 2:18 - loss: 2.748 - ETA: 2:16 - loss: 2.748 - ETA: 2:15 - loss: 2.748 - ETA: 2:14 - loss: 2.748 - ETA: 2:12 - loss: 2.747 - ETA: 2:11 - loss: 2.747 - ETA: 2:10 - loss: 2.747 - ETA: 2:10 - loss: 2.747 - ETA: 2:09 - loss: 2.747 - ETA: 2:07 - loss: 2.746 - ETA: 2:06 - loss: 2.746 - ETA: 2:05 - loss: 2.746 - ETA: 2:04 - loss: 2.746 - ETA: 2:03 - loss: 2.746 - ETA: 2:01 - loss: 2.746 - ETA: 2:00 - loss: 2.746 - ETA: 1:59 - loss: 2.745 - ETA: 1:58 - loss: 2.745 - ETA: 1:57 - loss: 2.745 - ETA: 1:55 - loss: 2.745 - ETA: 1:54 - loss: 2.745 - ETA: 1:53 - loss: 2.745 - ETA: 1:52 - loss: 2.744 - ETA: 1:51 - loss: 2.744 - ETA: 1:50 - loss: 2.744 - ETA: 1:49 - loss: 2.744 - ETA: 1:48 - loss: 2.744 - ETA: 1:47 - loss: 2.743 - ETA: 1:45 - loss: 2.743 - ETA: 1:44 - loss: 2.743 - ETA: 1:43 - loss: 2.742 - ETA: 1:42 - loss: 2.742 - ETA: 1:40 - loss: 2.742 - ETA: 1:39 - loss: 2.742 - ETA: 1:38 - loss: 2.742 - ETA: 1:37 - loss: 2.742 - ETA: 1:36 - loss: 2.741 - ETA: 1:34 - loss: 2.741 - ETA: 1:33 - loss: 2.741 - ETA: 1:32 - loss: 2.741 - ETA: 1:31 - loss: 2.741 - ETA: 1:29 - loss: 2.740 - ETA: 1:29 - loss: 2.740 - ETA: 1:27 - loss: 2.740 - ETA: 1:26 - loss: 2.740 - ETA: 1:25 - loss: 2.740 - ETA: 1:24 - loss: 2.740 - ETA: 1:23 - loss: 2.740 - ETA: 1:21 - loss: 2.740 - ETA: 1:20 - loss: 2.740 - ETA: 1:19 - loss: 2.740 - ETA: 1:18 - loss: 2.740 - ETA: 1:16 - loss: 2.740 - ETA: 1:15 - loss: 2.740 - ETA: 1:14 - loss: 2.740 - ETA: 1:12 - loss: 2.740 - ETA: 1:11 - loss: 2.740 - ETA: 1:10 - loss: 2.739 - ETA: 1:08 - loss: 2.739 - ETA: 1:07 - loss: 2.739 - ETA: 1:06 - loss: 2.739 - ETA: 1:05 - loss: 2.739 - ETA: 1:03 - loss: 2.739 - ETA: 1:02 - loss: 2.739 - ETA: 1:01 - loss: 2.739 - ETA: 59s - loss: 2.738 - ETA: 58s - loss: 2.73 - ETA: 57s - loss: 2.73 - ETA: 56s - loss: 2.73 - ETA: 54s - loss: 2.73 - ETA: 53s - loss: 2.73 - ETA: 52s - loss: 2.73 - ETA: 51s - loss: 2.73 - ETA: 49s - loss: 2.73 - ETA: 48s - loss: 2.73 - ETA: 47s - loss: 2.73 - ETA: 45s - loss: 2.73 - ETA: 44s - loss: 2.73 - ETA: 43s - loss: 2.73 - ETA: 42s - loss: 2.73 - ETA: 41s - loss: 2.73 - ETA: 39s - loss: 2.73 - ETA: 38s - loss: 2.73 - ETA: 37s - loss: 2.73 - ETA: 35s - loss: 2.73 - ETA: 34s - loss: 2.73 - ETA: 33s - loss: 2.73 - ETA: 32s - loss: 2.73 - ETA: 30s - loss: 2.73 - ETA: 29s - loss: 2.73 - ETA: 28s - loss: 2.73 - ETA: 26s - loss: 2.73 - ETA: 25s - loss: 2.73 - ETA: 24s - loss: 2.73 - ETA: 23s - loss: 2.73 - ETA: 21s - loss: 2.73 - ETA: 20s - loss: 2.74 - ETA: 19s - loss: 2.74 - ETA: 17s - loss: 2.74 - ETA: 16s - loss: 2.74 - ETA: 15s - loss: 2.74 - ETA: 14s - loss: 2.74 - ETA: 12s - loss: 2.73 - ETA: 11s - loss: 2.73 - ETA: 10s - loss: 2.73 - ETA: 9s - loss: 2.7395 - ETA: 7s - loss: 2.739 - ETA: 6s - loss: 2.739 - ETA: 5s - loss: 2.738 - ETA: 3s - loss: 2.738 - ETA: 2s - loss: 2.738 - ETA: 1s - loss: 2.738 - 397s 1s/step - loss: 2.7386 - val_loss: 2.7557\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.72076\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 5:30 - loss: 2.747 - ETA: 4:27 - loss: 2.741 - ETA: 4:08 - loss: 2.726 - ETA: 3:50 - loss: 2.735 - ETA: 3:26 - loss: 2.828 - ETA: 3:30 - loss: 2.823 - ETA: 3:35 - loss: 2.816 - ETA: 3:24 - loss: 2.820 - ETA: 3:20 - loss: 2.819 - ETA: 4:21 - loss: 2.808 - ETA: 5:12 - loss: 2.802 - ETA: 5:13 - loss: 2.803 - ETA: 5:56 - loss: 2.807 - ETA: 5:51 - loss: 2.806 - ETA: 5:55 - loss: 2.797 - ETA: 5:52 - loss: 2.798 - ETA: 5:52 - loss: 2.793 - ETA: 6:12 - loss: 2.790 - ETA: 6:00 - loss: 2.786 - ETA: 6:02 - loss: 2.783 - ETA: 5:54 - loss: 2.780 - ETA: 6:15 - loss: 2.779 - ETA: 6:09 - loss: 2.774 - ETA: 6:15 - loss: 2.773 - ETA: 6:15 - loss: 2.774 - ETA: 6:09 - loss: 2.769 - ETA: 6:05 - loss: 2.769 - ETA: 6:00 - loss: 2.766 - ETA: 5:56 - loss: 2.763 - ETA: 5:52 - loss: 2.762 - ETA: 5:48 - loss: 2.760 - ETA: 5:43 - loss: 2.757 - ETA: 5:45 - loss: 2.757 - ETA: 5:39 - loss: 2.762 - ETA: 5:35 - loss: 2.765 - ETA: 5:29 - loss: 2.765 - ETA: 5:26 - loss: 2.765 - ETA: 5:24 - loss: 2.768 - ETA: 5:29 - loss: 2.767 - ETA: 5:28 - loss: 2.768 - ETA: 5:24 - loss: 2.767 - ETA: 5:20 - loss: 2.770 - ETA: 5:26 - loss: 2.770 - ETA: 5:22 - loss: 2.769 - ETA: 5:30 - loss: 2.768 - ETA: 5:29 - loss: 2.769 - ETA: 5:31 - loss: 2.769 - ETA: 5:31 - loss: 2.768 - ETA: 5:27 - loss: 2.771 - ETA: 5:32 - loss: 2.770 - ETA: 5:29 - loss: 2.770 - ETA: 5:28 - loss: 2.771 - ETA: 5:30 - loss: 2.891 - ETA: 5:27 - loss: 2.896 - ETA: 5:28 - loss: 2.896 - ETA: 5:29 - loss: 2.903 - ETA: 5:25 - loss: 2.912 - ETA: 5:23 - loss: 2.916 - ETA: 5:20 - loss: 2.922 - ETA: 5:17 - loss: 2.933 - ETA: 5:14 - loss: 2.940 - ETA: 5:12 - loss: 2.944 - ETA: 5:12 - loss: 2.949 - ETA: 5:14 - loss: 2.953 - ETA: 5:14 - loss: 2.955 - ETA: 5:10 - loss: 2.956 - ETA: 5:07 - loss: 2.955 - ETA: 5:07 - loss: 2.954 - ETA: 5:04 - loss: 2.955 - ETA: 5:01 - loss: 2.954 - ETA: 4:59 - loss: 2.956 - ETA: 4:58 - loss: 2.956 - ETA: 4:55 - loss: 2.956 - ETA: 4:53 - loss: 2.956 - ETA: 4:52 - loss: 2.961 - ETA: 4:49 - loss: 2.961 - ETA: 4:48 - loss: 2.959 - ETA: 4:46 - loss: 2.958 - ETA: 4:44 - loss: 2.959 - ETA: 4:41 - loss: 2.959 - ETA: 4:39 - loss: 2.961 - ETA: 4:37 - loss: 2.961 - ETA: 4:38 - loss: 2.963 - ETA: 4:35 - loss: 2.963 - ETA: 4:34 - loss: 2.965 - ETA: 4:31 - loss: 2.964 - ETA: 4:29 - loss: 2.965 - ETA: 4:27 - loss: 2.966 - ETA: 4:28 - loss: 2.967 - ETA: 4:26 - loss: 2.965 - ETA: 4:26 - loss: 2.962 - ETA: 4:24 - loss: 2.963 - ETA: 4:22 - loss: 2.962 - ETA: 4:19 - loss: 2.965 - ETA: 4:20 - loss: 2.963 - ETA: 4:19 - loss: 2.963 - ETA: 4:19 - loss: 2.961 - ETA: 4:17 - loss: 2.960 - ETA: 4:16 - loss: 2.959 - ETA: 4:15 - loss: 2.958 - ETA: 4:12 - loss: 2.956 - ETA: 4:10 - loss: 2.955 - ETA: 4:09 - loss: 2.955 - ETA: 4:07 - loss: 2.953 - ETA: 4:05 - loss: 2.950 - ETA: 4:03 - loss: 2.949 - ETA: 4:01 - loss: 2.950 - ETA: 3:59 - loss: 2.948 - ETA: 3:59 - loss: 2.949 - ETA: 3:57 - loss: 2.948 - ETA: 3:56 - loss: 2.946 - ETA: 3:54 - loss: 2.946 - ETA: 3:53 - loss: 2.945 - ETA: 3:51 - loss: 2.943 - ETA: 3:50 - loss: 2.941 - ETA: 3:49 - loss: 2.940 - ETA: 3:48 - loss: 2.939 - ETA: 3:48 - loss: 2.938 - ETA: 3:46 - loss: 2.936 - ETA: 3:44 - loss: 2.935 - ETA: 3:43 - loss: 2.933 - ETA: 3:41 - loss: 2.932 - ETA: 3:40 - loss: 2.931 - ETA: 3:39 - loss: 2.929 - ETA: 3:37 - loss: 2.928 - ETA: 3:36 - loss: 2.927 - ETA: 3:35 - loss: 2.927 - ETA: 3:33 - loss: 2.929 - ETA: 3:32 - loss: 2.928 - ETA: 3:31 - loss: 2.926 - ETA: 3:32 - loss: 2.925 - ETA: 3:30 - loss: 2.926 - ETA: 3:29 - loss: 2.925 - ETA: 3:28 - loss: 2.924 - ETA: 3:27 - loss: 2.923 - ETA: 3:25 - loss: 2.922 - ETA: 3:24 - loss: 2.920 - ETA: 3:22 - loss: 2.918 - ETA: 3:21 - loss: 2.916 - ETA: 3:19 - loss: 2.915 - ETA: 3:18 - loss: 2.913 - ETA: 3:17 - loss: 2.912 - ETA: 3:15 - loss: 2.915 - ETA: 3:15 - loss: 2.914 - ETA: 3:14 - loss: 2.914 - ETA: 3:16 - loss: 2.915 - ETA: 3:15 - loss: 2.915 - ETA: 3:14 - loss: 2.918 - ETA: 3:13 - loss: 2.918 - ETA: 3:12 - loss: 2.918 - ETA: 3:11 - loss: 2.919 - ETA: 3:09 - loss: 2.919 - ETA: 3:08 - loss: 2.923 - ETA: 3:07 - loss: 2.923 - ETA: 3:07 - loss: 2.923 - ETA: 3:07 - loss: 2.923 - ETA: 3:05 - loss: 2.922 - ETA: 3:04 - loss: 2.921 - ETA: 3:03 - loss: 2.921 - ETA: 3:02 - loss: 2.920 - ETA: 3:01 - loss: 2.919 - ETA: 3:00 - loss: 2.918 - ETA: 2:59 - loss: 2.919 - ETA: 2:58 - loss: 2.918 - ETA: 2:57 - loss: 2.917 - ETA: 2:56 - loss: 2.916 - ETA: 2:55 - loss: 2.915 - ETA: 2:54 - loss: 2.914 - ETA: 2:54 - loss: 2.913 - ETA: 2:52 - loss: 2.911 - ETA: 2:51 - loss: 2.911 - ETA: 2:50 - loss: 2.914 - ETA: 2:49 - loss: 2.913 - ETA: 2:47 - loss: 2.912 - ETA: 2:47 - loss: 2.911 - ETA: 2:45 - loss: 2.910 - ETA: 2:44 - loss: 2.909 - ETA: 2:43 - loss: 2.909 - ETA: 2:42 - loss: 2.908 - ETA: 2:40 - loss: 2.910 - ETA: 2:39 - loss: 2.909 - ETA: 2:37 - loss: 2.909 - ETA: 2:36 - loss: 2.909 - ETA: 2:34 - loss: 2.908 - ETA: 2:33 - loss: 2.907 - ETA: 2:31 - loss: 2.906 - ETA: 2:30 - loss: 2.905 - ETA: 2:29 - loss: 2.905 - ETA: 2:27 - loss: 2.904 - ETA: 2:26 - loss: 2.902 - ETA: 2:24 - loss: 2.901 - ETA: 2:22 - loss: 2.900 - ETA: 2:22 - loss: 2.899 - ETA: 2:20 - loss: 2.898 - ETA: 2:18 - loss: 2.897 - ETA: 2:17 - loss: 2.897 - ETA: 2:16 - loss: 2.897 - ETA: 2:14 - loss: 2.896 - ETA: 2:13 - loss: 2.894 - ETA: 2:12 - loss: 2.893 - ETA: 2:10 - loss: 2.893 - ETA: 2:09 - loss: 2.892 - ETA: 2:08 - loss: 2.891 - ETA: 2:06 - loss: 2.890 - ETA: 2:05 - loss: 2.889 - ETA: 2:03 - loss: 2.888 - ETA: 2:02 - loss: 2.888 - ETA: 2:01 - loss: 2.887 - ETA: 2:00 - loss: 2.886 - ETA: 1:59 - loss: 2.885 - ETA: 1:58 - loss: 2.884 - ETA: 1:56 - loss: 2.883 - ETA: 1:55 - loss: 2.882 - ETA: 1:55 - loss: 2.881 - ETA: 1:53 - loss: 2.880 - ETA: 1:52 - loss: 2.880 - ETA: 1:50 - loss: 2.879 - ETA: 1:49 - loss: 2.878 - ETA: 1:47 - loss: 2.878 - ETA: 1:46 - loss: 2.877 - ETA: 1:45 - loss: 2.876 - ETA: 1:43 - loss: 2.875 - ETA: 1:42 - loss: 2.874 - ETA: 1:40 - loss: 2.873 - ETA: 1:40 - loss: 2.872 - ETA: 1:38 - loss: 2.871 - ETA: 1:37 - loss: 2.871 - ETA: 1:35 - loss: 2.870 - ETA: 1:34 - loss: 2.870 - ETA: 1:33 - loss: 2.870 - ETA: 1:31 - loss: 2.869 - ETA: 1:30 - loss: 2.869 - ETA: 1:29 - loss: 2.867 - ETA: 1:27 - loss: 2.867 - ETA: 1:26 - loss: 2.867 - ETA: 1:25 - loss: 2.866 - ETA: 1:23 - loss: 2.865 - ETA: 1:22 - loss: 2.866 - ETA: 1:20 - loss: 2.865 - ETA: 1:19 - loss: 2.865 - ETA: 1:17 - loss: 2.865 - ETA: 1:16 - loss: 2.864 - ETA: 1:15 - loss: 2.863 - ETA: 1:14 - loss: 2.863 - ETA: 1:12 - loss: 2.864 - ETA: 1:11 - loss: 2.863 - ETA: 1:09 - loss: 2.862 - ETA: 1:08 - loss: 2.862 - ETA: 1:07 - loss: 2.861 - ETA: 1:05 - loss: 2.862 - ETA: 1:04 - loss: 2.861 - ETA: 1:03 - loss: 2.861 - ETA: 1:01 - loss: 2.860 - ETA: 1:00 - loss: 2.860 - ETA: 59s - loss: 2.859 - ETA: 57s - loss: 2.85 - ETA: 56s - loss: 2.85 - ETA: 55s - loss: 2.85 - ETA: 54s - loss: 2.85 - ETA: 52s - loss: 2.85 - ETA: 51s - loss: 2.85 - ETA: 49s - loss: 2.85 - ETA: 48s - loss: 2.85 - ETA: 47s - loss: 2.85 - ETA: 46s - loss: 2.85 - ETA: 44s - loss: 2.85 - ETA: 43s - loss: 2.85 - ETA: 42s - loss: 2.85 - ETA: 40s - loss: 2.85 - ETA: 39s - loss: 2.85 - ETA: 38s - loss: 2.84 - ETA: 36s - loss: 2.84 - ETA: 35s - loss: 2.84 - ETA: 34s - loss: 2.84 - ETA: 32s - loss: 2.84 - ETA: 31s - loss: 2.84 - ETA: 30s - loss: 2.84 - ETA: 28s - loss: 2.84 - ETA: 27s - loss: 2.84 - ETA: 26s - loss: 2.84 - ETA: 24s - loss: 2.84 - ETA: 23s - loss: 2.84 - ETA: 22s - loss: 2.84 - ETA: 20s - loss: 2.84 - ETA: 19s - loss: 2.84 - ETA: 18s - loss: 2.84 - ETA: 16s - loss: 2.84 - ETA: 15s - loss: 2.84 - ETA: 14s - loss: 2.84 - ETA: 13s - loss: 2.84 - ETA: 11s - loss: 2.83 - ETA: 10s - loss: 2.83 - ETA: 9s - loss: 2.8387 - ETA: 7s - loss: 2.838 - ETA: 6s - loss: 2.837 - ETA: 5s - loss: 2.837 - ETA: 3s - loss: 2.837 - ETA: 2s - loss: 2.836 - ETA: 1s - loss: 2.836 - 406s 1s/step - loss: 2.8360 - val_loss: 2.7891\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.72076\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 2:47 - loss: 2.734 - ETA: 3:01 - loss: 2.727 - ETA: 3:42 - loss: 2.703 - ETA: 4:01 - loss: 2.747 - ETA: 3:48 - loss: 2.718 - ETA: 4:25 - loss: 2.729 - ETA: 4:42 - loss: 2.733 - ETA: 4:57 - loss: 2.728 - ETA: 5:26 - loss: 2.735 - ETA: 5:42 - loss: 2.739 - ETA: 5:54 - loss: 2.737 - ETA: 6:21 - loss: 2.730 - ETA: 6:32 - loss: 2.727 - ETA: 6:34 - loss: 2.734 - ETA: 7:01 - loss: 2.729 - ETA: 6:53 - loss: 2.728 - ETA: 7:07 - loss: 2.722 - ETA: 7:05 - loss: 2.724 - ETA: 7:02 - loss: 2.722 - ETA: 6:54 - loss: 2.723 - ETA: 6:52 - loss: 2.723 - ETA: 7:06 - loss: 2.719 - ETA: 7:19 - loss: 2.720 - ETA: 7:16 - loss: 2.719 - ETA: 7:10 - loss: 2.717 - ETA: 7:10 - loss: 2.722 - ETA: 7:10 - loss: 2.720 - ETA: 7:22 - loss: 2.719 - ETA: 7:14 - loss: 2.719 - ETA: 7:17 - loss: 2.718 - ETA: 7:19 - loss: 2.718 - ETA: 7:16 - loss: 2.718 - ETA: 7:22 - loss: 2.717 - ETA: 7:24 - loss: 2.715 - ETA: 7:16 - loss: 2.713 - ETA: 7:17 - loss: 2.716 - ETA: 7:16 - loss: 2.716 - ETA: 7:17 - loss: 2.714 - ETA: 7:16 - loss: 2.713 - ETA: 7:09 - loss: 2.713 - ETA: 7:06 - loss: 2.715 - ETA: 7:05 - loss: 2.714 - ETA: 6:56 - loss: 2.718 - ETA: 6:56 - loss: 2.717 - ETA: 6:49 - loss: 2.718 - ETA: 6:46 - loss: 2.722 - ETA: 6:40 - loss: 2.721 - ETA: 6:37 - loss: 2.723 - ETA: 6:33 - loss: 2.725 - ETA: 6:30 - loss: 2.723 - ETA: 6:27 - loss: 2.722 - ETA: 6:24 - loss: 2.723 - ETA: 6:21 - loss: 2.724 - ETA: 6:20 - loss: 2.723 - ETA: 6:19 - loss: 2.723 - ETA: 6:18 - loss: 2.722 - ETA: 6:17 - loss: 2.722 - ETA: 6:20 - loss: 2.721 - ETA: 6:19 - loss: 2.721 - ETA: 6:20 - loss: 2.719 - ETA: 6:22 - loss: 2.718 - ETA: 6:27 - loss: 2.718 - ETA: 6:26 - loss: 2.717 - ETA: 6:24 - loss: 2.715 - ETA: 6:23 - loss: 2.715 - ETA: 6:21 - loss: 2.714 - ETA: 6:21 - loss: 2.715 - ETA: 6:20 - loss: 2.714 - ETA: 6:19 - loss: 2.715 - ETA: 6:17 - loss: 2.713 - ETA: 6:16 - loss: 2.713 - ETA: 6:17 - loss: 2.712 - ETA: 6:15 - loss: 2.715 - ETA: 6:14 - loss: 2.714 - ETA: 6:12 - loss: 2.714 - ETA: 6:10 - loss: 2.714 - ETA: 6:09 - loss: 2.714 - ETA: 6:04 - loss: 2.714 - ETA: 6:06 - loss: 2.713 - ETA: 6:04 - loss: 2.712 - ETA: 6:01 - loss: 2.712 - ETA: 5:58 - loss: 2.711 - ETA: 5:55 - loss: 2.711 - ETA: 5:52 - loss: 2.710 - ETA: 5:50 - loss: 2.709 - ETA: 5:47 - loss: 2.712 - ETA: 5:44 - loss: 2.711 - ETA: 5:41 - loss: 2.712 - ETA: 5:38 - loss: 2.712 - ETA: 5:37 - loss: 2.718 - ETA: 5:35 - loss: 2.717 - ETA: 5:35 - loss: 2.719 - ETA: 5:31 - loss: 2.718 - ETA: 5:28 - loss: 2.718 - ETA: 5:26 - loss: 2.719 - ETA: 5:23 - loss: 2.719 - ETA: 5:21 - loss: 2.718 - ETA: 5:18 - loss: 2.717 - ETA: 5:16 - loss: 2.718 - ETA: 5:14 - loss: 2.719 - ETA: 5:11 - loss: 2.720 - ETA: 5:11 - loss: 2.720 - ETA: 5:10 - loss: 2.719 - ETA: 5:08 - loss: 2.720 - ETA: 5:05 - loss: 2.720 - ETA: 5:03 - loss: 2.720 - ETA: 5:00 - loss: 2.720 - ETA: 4:58 - loss: 2.720 - ETA: 4:56 - loss: 2.720 - ETA: 4:55 - loss: 2.720 - ETA: 4:52 - loss: 2.719 - ETA: 4:49 - loss: 2.718 - ETA: 4:48 - loss: 2.718 - ETA: 4:46 - loss: 2.718 - ETA: 4:43 - loss: 2.717 - ETA: 4:42 - loss: 2.716 - ETA: 4:41 - loss: 2.716 - ETA: 4:39 - loss: 2.715 - ETA: 4:37 - loss: 2.714 - ETA: 4:35 - loss: 2.713 - ETA: 4:34 - loss: 2.713 - ETA: 4:33 - loss: 2.712 - ETA: 4:31 - loss: 2.711 - ETA: 4:30 - loss: 2.711 - ETA: 4:27 - loss: 2.710 - ETA: 4:26 - loss: 2.709 - ETA: 4:26 - loss: 2.710 - ETA: 4:24 - loss: 2.709 - ETA: 4:22 - loss: 2.708 - ETA: 4:21 - loss: 2.708 - ETA: 4:18 - loss: 2.708 - ETA: 4:16 - loss: 2.708 - ETA: 4:15 - loss: 2.708 - ETA: 4:12 - loss: 2.707 - ETA: 4:11 - loss: 2.706 - ETA: 4:09 - loss: 2.706 - ETA: 4:09 - loss: 2.706 - ETA: 4:08 - loss: 2.705 - ETA: 4:08 - loss: 2.705 - ETA: 4:07 - loss: 2.704 - ETA: 4:07 - loss: 2.703 - ETA: 4:07 - loss: 2.703 - ETA: 4:06 - loss: 2.703 - ETA: 4:05 - loss: 2.702 - ETA: 4:05 - loss: 2.702 - ETA: 4:04 - loss: 2.701 - ETA: 4:03 - loss: 2.701 - ETA: 4:02 - loss: 2.700 - ETA: 4:01 - loss: 2.700 - ETA: 3:59 - loss: 2.699 - ETA: 3:58 - loss: 2.699 - ETA: 3:57 - loss: 2.699 - ETA: 3:56 - loss: 2.698 - ETA: 3:55 - loss: 2.698 - ETA: 3:53 - loss: 2.699 - ETA: 3:51 - loss: 2.698 - ETA: 3:50 - loss: 2.698 - ETA: 3:49 - loss: 2.698 - ETA: 3:48 - loss: 2.698 - ETA: 3:47 - loss: 2.699 - ETA: 3:46 - loss: 2.699 - ETA: 3:45 - loss: 2.699 - ETA: 3:43 - loss: 2.698 - ETA: 3:42 - loss: 2.698 - ETA: 3:40 - loss: 2.698 - ETA: 3:38 - loss: 2.697 - ETA: 3:36 - loss: 2.697 - ETA: 3:35 - loss: 2.697 - ETA: 3:33 - loss: 2.696 - ETA: 3:32 - loss: 2.696 - ETA: 3:31 - loss: 2.696 - ETA: 3:30 - loss: 2.696 - ETA: 3:29 - loss: 2.696 - ETA: 3:28 - loss: 2.695 - ETA: 3:26 - loss: 2.695 - ETA: 3:25 - loss: 2.695 - ETA: 3:25 - loss: 2.694 - ETA: 3:23 - loss: 2.694 - ETA: 3:22 - loss: 2.694 - ETA: 3:20 - loss: 2.693 - ETA: 3:18 - loss: 2.693 - ETA: 3:17 - loss: 2.692 - ETA: 3:16 - loss: 2.692 - ETA: 3:15 - loss: 2.692 - ETA: 3:14 - loss: 2.692 - ETA: 3:13 - loss: 2.691 - ETA: 3:11 - loss: 2.691 - ETA: 3:11 - loss: 2.691 - ETA: 3:09 - loss: 2.691 - ETA: 3:08 - loss: 2.690 - ETA: 3:08 - loss: 2.690 - ETA: 3:07 - loss: 2.689 - ETA: 3:06 - loss: 2.689 - ETA: 3:04 - loss: 2.689 - ETA: 3:03 - loss: 2.688 - ETA: 3:02 - loss: 2.688 - ETA: 3:01 - loss: 2.688 - ETA: 3:00 - loss: 2.687 - ETA: 2:59 - loss: 2.687 - ETA: 2:57 - loss: 2.687 - ETA: 2:56 - loss: 2.686 - ETA: 2:55 - loss: 2.687 - ETA: 2:53 - loss: 2.686 - ETA: 2:52 - loss: 2.686 - ETA: 2:51 - loss: 2.685 - ETA: 2:50 - loss: 2.685 - ETA: 2:48 - loss: 2.685 - ETA: 2:46 - loss: 2.684 - ETA: 2:45 - loss: 2.685 - ETA: 2:43 - loss: 2.685 - ETA: 2:42 - loss: 2.685 - ETA: 2:40 - loss: 2.685 - ETA: 2:39 - loss: 2.685 - ETA: 2:37 - loss: 2.685 - ETA: 2:35 - loss: 2.685 - ETA: 2:34 - loss: 2.685 - ETA: 2:32 - loss: 2.685 - ETA: 2:30 - loss: 2.685 - ETA: 2:28 - loss: 2.685 - ETA: 2:26 - loss: 2.684 - ETA: 2:24 - loss: 2.684 - ETA: 2:22 - loss: 2.684 - ETA: 2:21 - loss: 2.684 - ETA: 2:19 - loss: 2.684 - ETA: 2:17 - loss: 2.683 - ETA: 2:16 - loss: 2.683 - ETA: 2:14 - loss: 2.683 - ETA: 2:12 - loss: 2.683 - ETA: 2:10 - loss: 2.683 - ETA: 2:09 - loss: 2.682 - ETA: 2:07 - loss: 2.683 - ETA: 2:05 - loss: 2.683 - ETA: 2:03 - loss: 2.683 - ETA: 2:02 - loss: 2.683 - ETA: 2:00 - loss: 2.682 - ETA: 1:58 - loss: 2.682 - ETA: 1:56 - loss: 2.682 - ETA: 1:54 - loss: 2.682 - ETA: 1:53 - loss: 2.682 - ETA: 1:51 - loss: 2.682 - ETA: 1:49 - loss: 2.682 - ETA: 1:48 - loss: 2.681 - ETA: 1:46 - loss: 2.682 - ETA: 1:44 - loss: 2.681 - ETA: 1:42 - loss: 2.681 - ETA: 1:40 - loss: 2.681 - ETA: 1:39 - loss: 2.680 - ETA: 1:37 - loss: 2.680 - ETA: 1:35 - loss: 2.680 - ETA: 1:33 - loss: 2.680 - ETA: 1:31 - loss: 2.680 - ETA: 1:30 - loss: 2.681 - ETA: 1:28 - loss: 2.680 - ETA: 1:26 - loss: 2.681 - ETA: 1:24 - loss: 2.681 - ETA: 1:22 - loss: 2.681 - ETA: 1:20 - loss: 2.681 - ETA: 1:19 - loss: 2.681 - ETA: 1:17 - loss: 2.681 - ETA: 1:15 - loss: 2.681 - ETA: 1:13 - loss: 2.681 - ETA: 1:11 - loss: 2.680 - ETA: 1:10 - loss: 2.680 - ETA: 1:08 - loss: 2.680 - ETA: 1:06 - loss: 2.680 - ETA: 1:04 - loss: 2.680 - ETA: 1:02 - loss: 2.680 - ETA: 1:01 - loss: 2.680 - ETA: 59s - loss: 2.680 - ETA: 57s - loss: 2.67 - ETA: 55s - loss: 2.67 - ETA: 53s - loss: 2.67 - ETA: 51s - loss: 2.67 - ETA: 49s - loss: 2.67 - ETA: 47s - loss: 2.67 - ETA: 45s - loss: 2.67 - ETA: 44s - loss: 2.67 - ETA: 42s - loss: 2.67 - ETA: 40s - loss: 2.67 - ETA: 38s - loss: 2.67 - ETA: 36s - loss: 2.67 - ETA: 34s - loss: 2.67 - ETA: 32s - loss: 2.67 - ETA: 30s - loss: 2.67 - ETA: 28s - loss: 2.67 - ETA: 26s - loss: 2.67 - ETA: 25s - loss: 2.67 - ETA: 23s - loss: 2.67 - ETA: 21s - loss: 2.67 - ETA: 19s - loss: 2.67 - ETA: 17s - loss: 2.67 - ETA: 15s - loss: 2.67 - ETA: 13s - loss: 2.67 - ETA: 11s - loss: 2.67 - ETA: 9s - loss: 2.6776 - ETA: 7s - loss: 2.677 - ETA: 5s - loss: 2.677 - ETA: 3s - loss: 2.677 - ETA: 1s - loss: 2.676 - 588s 2s/step - loss: 2.6772 - val_loss: 2.7549\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.72076\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 9:39 - loss: 2.750 - ETA: 11:12 - loss: 2.77 - ETA: 10:31 - loss: 2.73 - ETA: 11:12 - loss: 2.76 - ETA: 10:34 - loss: 2.74 - ETA: 10:09 - loss: 2.75 - ETA: 10:04 - loss: 2.76 - ETA: 10:00 - loss: 2.78 - ETA: 10:08 - loss: 2.78 - ETA: 9:53 - loss: 2.7701 - ETA: 10:13 - loss: 2.76 - ETA: 10:23 - loss: 2.75 - ETA: 10:40 - loss: 2.74 - ETA: 10:28 - loss: 2.74 - ETA: 10:36 - loss: 2.75 - ETA: 10:27 - loss: 2.74 - ETA: 10:19 - loss: 2.74 - ETA: 10:11 - loss: 2.73 - ETA: 10:01 - loss: 2.73 - ETA: 10:01 - loss: 2.72 - ETA: 10:07 - loss: 2.72 - ETA: 9:57 - loss: 2.7260 - ETA: 9:50 - loss: 2.725 - ETA: 9:44 - loss: 2.721 - ETA: 9:35 - loss: 2.724 - ETA: 9:42 - loss: 2.722 - ETA: 9:47 - loss: 2.720 - ETA: 9:48 - loss: 2.721 - ETA: 9:45 - loss: 2.720 - ETA: 9:44 - loss: 2.719 - ETA: 9:39 - loss: 2.716 - ETA: 9:38 - loss: 2.716 - ETA: 9:36 - loss: 2.714 - ETA: 9:30 - loss: 2.712 - ETA: 9:28 - loss: 2.713 - ETA: 9:26 - loss: 2.712 - ETA: 9:26 - loss: 2.711 - ETA: 9:26 - loss: 2.710 - ETA: 9:24 - loss: 2.709 - ETA: 9:15 - loss: 2.716 - ETA: 9:15 - loss: 2.713 - ETA: 9:15 - loss: 2.713 - ETA: 9:14 - loss: 2.713 - ETA: 9:11 - loss: 2.711 - ETA: 9:12 - loss: 2.708 - ETA: 9:10 - loss: 2.710 - ETA: 9:14 - loss: 2.710 - ETA: 9:11 - loss: 2.710 - ETA: 9:08 - loss: 2.708 - ETA: 9:03 - loss: 2.707 - ETA: 8:58 - loss: 2.708 - ETA: 8:55 - loss: 2.711 - ETA: 8:57 - loss: 2.710 - ETA: 8:55 - loss: 2.708 - ETA: 8:51 - loss: 2.707 - ETA: 8:51 - loss: 2.706 - ETA: 8:47 - loss: 2.708 - ETA: 8:45 - loss: 2.706 - ETA: 8:46 - loss: 2.705 - ETA: 8:47 - loss: 2.705 - ETA: 8:44 - loss: 2.703 - ETA: 8:41 - loss: 2.704 - ETA: 8:37 - loss: 2.704 - ETA: 8:36 - loss: 2.704 - ETA: 8:31 - loss: 2.703 - ETA: 8:27 - loss: 2.701 - ETA: 8:23 - loss: 2.701 - ETA: 8:22 - loss: 2.699 - ETA: 8:18 - loss: 2.698 - ETA: 8:16 - loss: 2.697 - ETA: 8:12 - loss: 2.696 - ETA: 8:10 - loss: 2.695 - ETA: 8:06 - loss: 2.694 - ETA: 8:03 - loss: 2.693 - ETA: 8:01 - loss: 2.692 - ETA: 8:00 - loss: 2.691 - ETA: 7:57 - loss: 2.690 - ETA: 7:53 - loss: 2.688 - ETA: 7:50 - loss: 2.687 - ETA: 7:48 - loss: 2.687 - ETA: 7:44 - loss: 2.686 - ETA: 7:41 - loss: 2.685 - ETA: 7:39 - loss: 2.684 - ETA: 7:39 - loss: 2.683 - ETA: 7:35 - loss: 2.682 - ETA: 7:32 - loss: 2.683 - ETA: 7:28 - loss: 2.682 - ETA: 7:25 - loss: 2.682 - ETA: 7:21 - loss: 2.681 - ETA: 7:20 - loss: 2.681 - ETA: 7:17 - loss: 2.680 - ETA: 7:14 - loss: 2.679 - ETA: 7:13 - loss: 2.678 - ETA: 7:10 - loss: 2.676 - ETA: 7:09 - loss: 2.676 - ETA: 7:05 - loss: 2.676 - ETA: 7:03 - loss: 2.675 - ETA: 7:01 - loss: 2.674 - ETA: 6:58 - loss: 2.674 - ETA: 6:57 - loss: 2.672 - ETA: 6:54 - loss: 2.672 - ETA: 6:51 - loss: 2.671 - ETA: 6:48 - loss: 2.670 - ETA: 6:46 - loss: 2.670 - ETA: 6:44 - loss: 2.669 - ETA: 6:41 - loss: 2.669 - ETA: 6:38 - loss: 2.669 - ETA: 6:36 - loss: 2.669 - ETA: 6:33 - loss: 2.669 - ETA: 6:31 - loss: 2.669 - ETA: 6:29 - loss: 2.669 - ETA: 6:28 - loss: 2.669 - ETA: 6:26 - loss: 2.668 - ETA: 6:24 - loss: 2.669 - ETA: 6:21 - loss: 2.669 - ETA: 6:19 - loss: 2.668 - ETA: 6:18 - loss: 2.668 - ETA: 6:17 - loss: 2.667 - ETA: 6:15 - loss: 2.667 - ETA: 6:13 - loss: 2.666 - ETA: 6:11 - loss: 2.665 - ETA: 6:10 - loss: 2.666 - ETA: 6:07 - loss: 2.665 - ETA: 6:04 - loss: 2.664 - ETA: 6:02 - loss: 2.664 - ETA: 6:00 - loss: 2.664 - ETA: 5:57 - loss: 2.664 - ETA: 5:55 - loss: 2.664 - ETA: 5:52 - loss: 2.664 - ETA: 5:50 - loss: 2.665 - ETA: 5:47 - loss: 2.664 - ETA: 5:45 - loss: 2.665 - ETA: 5:42 - loss: 2.665 - ETA: 5:41 - loss: 2.665 - ETA: 5:39 - loss: 2.664 - ETA: 5:37 - loss: 2.663 - ETA: 5:35 - loss: 2.663 - ETA: 5:33 - loss: 2.664 - ETA: 5:30 - loss: 2.665 - ETA: 5:29 - loss: 2.665 - ETA: 5:27 - loss: 2.665 - ETA: 5:25 - loss: 2.665 - ETA: 5:23 - loss: 2.665 - ETA: 5:21 - loss: 2.664 - ETA: 5:19 - loss: 2.664 - ETA: 5:17 - loss: 2.664 - ETA: 5:17 - loss: 2.664 - ETA: 5:15 - loss: 2.664 - ETA: 5:14 - loss: 2.664 - ETA: 5:13 - loss: 2.664 - ETA: 5:12 - loss: 2.663 - ETA: 5:12 - loss: 2.663 - ETA: 5:10 - loss: 2.663 - ETA: 5:08 - loss: 2.662 - ETA: 5:07 - loss: 2.662 - ETA: 5:04 - loss: 2.662 - ETA: 5:03 - loss: 2.662 - ETA: 5:00 - loss: 2.662 - ETA: 4:59 - loss: 2.662 - ETA: 4:57 - loss: 2.661 - ETA: 4:55 - loss: 2.661 - ETA: 4:54 - loss: 2.660 - ETA: 4:51 - loss: 2.660 - ETA: 4:49 - loss: 2.660 - ETA: 4:47 - loss: 2.660 - ETA: 4:45 - loss: 2.660 - ETA: 4:43 - loss: 2.659 - ETA: 4:41 - loss: 2.659 - ETA: 4:39 - loss: 2.658 - ETA: 4:36 - loss: 2.658 - ETA: 4:34 - loss: 2.657 - ETA: 4:32 - loss: 2.657 - ETA: 4:30 - loss: 2.656 - ETA: 4:28 - loss: 2.656 - ETA: 4:26 - loss: 2.656 - ETA: 4:24 - loss: 2.656 - ETA: 4:21 - loss: 2.655 - ETA: 4:19 - loss: 2.655 - ETA: 4:18 - loss: 2.655 - ETA: 4:15 - loss: 2.656 - ETA: 4:13 - loss: 2.656 - ETA: 4:11 - loss: 2.656 - ETA: 4:10 - loss: 2.656 - ETA: 4:07 - loss: 2.657 - ETA: 4:05 - loss: 2.656 - ETA: 4:03 - loss: 2.656 - ETA: 4:01 - loss: 2.656 - ETA: 3:59 - loss: 2.657 - ETA: 3:57 - loss: 2.656 - ETA: 3:56 - loss: 2.656 - ETA: 3:54 - loss: 2.656 - ETA: 3:52 - loss: 2.656 - ETA: 3:50 - loss: 2.656 - ETA: 3:48 - loss: 2.656 - ETA: 3:45 - loss: 2.656 - ETA: 3:43 - loss: 2.655 - ETA: 3:41 - loss: 2.655 - ETA: 3:39 - loss: 2.655 - ETA: 3:36 - loss: 2.655 - ETA: 3:34 - loss: 2.654 - ETA: 3:32 - loss: 2.654 - ETA: 3:30 - loss: 2.653 - ETA: 3:27 - loss: 2.653 - ETA: 3:25 - loss: 2.653 - ETA: 3:23 - loss: 2.654 - ETA: 3:20 - loss: 2.653 - ETA: 3:18 - loss: 2.653 - ETA: 3:16 - loss: 2.653 - ETA: 3:14 - loss: 2.653 - ETA: 3:12 - loss: 2.653 - ETA: 3:10 - loss: 2.653 - ETA: 3:07 - loss: 2.653 - ETA: 3:05 - loss: 2.653 - ETA: 3:03 - loss: 2.653 - ETA: 3:01 - loss: 2.653 - ETA: 2:59 - loss: 2.653 - ETA: 2:57 - loss: 2.653 - ETA: 2:55 - loss: 2.654 - ETA: 2:53 - loss: 2.653 - ETA: 2:51 - loss: 2.653 - ETA: 2:49 - loss: 2.653 - ETA: 2:47 - loss: 2.653 - ETA: 2:45 - loss: 2.654 - ETA: 2:43 - loss: 2.655 - ETA: 2:41 - loss: 2.656 - ETA: 2:39 - loss: 2.656 - ETA: 2:37 - loss: 2.656 - ETA: 2:35 - loss: 2.656 - ETA: 2:32 - loss: 2.656 - ETA: 2:30 - loss: 2.656 - ETA: 2:28 - loss: 2.656 - ETA: 2:26 - loss: 2.656 - ETA: 2:24 - loss: 2.656 - ETA: 2:22 - loss: 2.658 - ETA: 2:20 - loss: 2.658 - ETA: 2:18 - loss: 2.657 - ETA: 2:16 - loss: 2.657 - ETA: 2:14 - loss: 2.657 - ETA: 2:12 - loss: 2.657 - ETA: 2:09 - loss: 2.657 - ETA: 2:08 - loss: 2.657 - ETA: 2:05 - loss: 2.657 - ETA: 2:03 - loss: 2.657 - ETA: 2:01 - loss: 2.656 - ETA: 1:59 - loss: 2.656 - ETA: 1:57 - loss: 2.656 - ETA: 1:55 - loss: 2.656 - ETA: 1:53 - loss: 2.655 - ETA: 1:50 - loss: 2.655 - ETA: 1:48 - loss: 2.655 - ETA: 1:46 - loss: 2.654 - ETA: 1:44 - loss: 2.654 - ETA: 1:42 - loss: 2.654 - ETA: 1:40 - loss: 2.654 - ETA: 1:37 - loss: 2.653 - ETA: 1:35 - loss: 2.653 - ETA: 1:33 - loss: 2.653 - ETA: 1:31 - loss: 2.653 - ETA: 1:29 - loss: 2.653 - ETA: 1:27 - loss: 2.653 - ETA: 1:24 - loss: 2.653 - ETA: 1:22 - loss: 2.652 - ETA: 1:20 - loss: 2.652 - ETA: 1:18 - loss: 2.652 - ETA: 1:16 - loss: 2.651 - ETA: 1:13 - loss: 2.651 - ETA: 1:11 - loss: 2.651 - ETA: 1:09 - loss: 2.650 - ETA: 1:07 - loss: 2.650 - ETA: 1:05 - loss: 2.650 - ETA: 1:02 - loss: 2.650 - ETA: 1:00 - loss: 2.650 - ETA: 58s - loss: 2.650 - ETA: 56s - loss: 2.65 - ETA: 54s - loss: 2.64 - ETA: 52s - loss: 2.65 - ETA: 49s - loss: 2.64 - ETA: 47s - loss: 2.64 - ETA: 45s - loss: 2.64 - ETA: 43s - loss: 2.64 - ETA: 41s - loss: 2.64 - ETA: 38s - loss: 2.64 - ETA: 36s - loss: 2.64 - ETA: 34s - loss: 2.64 - ETA: 32s - loss: 2.64 - ETA: 30s - loss: 2.64 - ETA: 28s - loss: 2.64 - ETA: 25s - loss: 2.64 - ETA: 23s - loss: 2.64 - ETA: 21s - loss: 2.64 - ETA: 19s - loss: 2.64 - ETA: 17s - loss: 2.64 - ETA: 15s - loss: 2.64 - ETA: 12s - loss: 2.64 - ETA: 10s - loss: 2.64 - ETA: 8s - loss: 2.6473 - ETA: 6s - loss: 2.647 - ETA: 4s - loss: 2.647 - ETA: 2s - loss: 2.646 - 657s 2s/step - loss: 2.6467 - val_loss: 2.6248\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.72076 to 2.62484, saving model to ssd7_epoch-09_loss-2.6463_val_loss-2.6248.h5\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/300 [===========>..................] - ETA: 14:44 - loss: 2.58 - ETA: 11:20 - loss: 2.59 - ETA: 11:34 - loss: 2.60 - ETA: 12:23 - loss: 2.60 - ETA: 12:55 - loss: 2.62 - ETA: 11:37 - loss: 2.61 - ETA: 11:10 - loss: 2.61 - ETA: 10:42 - loss: 2.62 - ETA: 10:42 - loss: 2.62 - ETA: 10:23 - loss: 2.62 - ETA: 10:07 - loss: 2.62 - ETA: 10:05 - loss: 2.62 - ETA: 9:51 - loss: 2.6175 - ETA: 9:51 - loss: 2.624 - ETA: 9:41 - loss: 2.630 - ETA: 9:45 - loss: 2.631 - ETA: 9:45 - loss: 2.637 - ETA: 9:44 - loss: 2.635 - ETA: 9:46 - loss: 2.641 - ETA: 9:43 - loss: 2.652 - ETA: 9:35 - loss: 2.671 - ETA: 9:21 - loss: 2.677 - ETA: 9:15 - loss: 2.675 - ETA: 9:14 - loss: 2.679 - ETA: 9:09 - loss: 2.683 - ETA: 9:08 - loss: 2.686 - ETA: 9:07 - loss: 2.687 - ETA: 9:11 - loss: 2.690 - ETA: 9:10 - loss: 2.689 - ETA: 9:10 - loss: 2.685 - ETA: 9:04 - loss: 2.682 - ETA: 8:57 - loss: 2.679 - ETA: 8:56 - loss: 2.682 - ETA: 8:55 - loss: 2.682 - ETA: 8:49 - loss: 2.681 - ETA: 8:50 - loss: 2.681 - ETA: 8:50 - loss: 2.681 - ETA: 8:52 - loss: 2.679 - ETA: 8:51 - loss: 2.676 - ETA: 8:50 - loss: 2.676 - ETA: 8:48 - loss: 2.674 - ETA: 8:47 - loss: 2.673 - ETA: 8:45 - loss: 2.677 - ETA: 8:42 - loss: 2.675 - ETA: 8:38 - loss: 2.675 - ETA: 8:37 - loss: 2.675 - ETA: 8:32 - loss: 2.674 - ETA: 8:32 - loss: 2.672 - ETA: 8:31 - loss: 2.671 - ETA: 8:27 - loss: 2.670 - ETA: 8:22 - loss: 2.670 - ETA: 8:19 - loss: 2.669 - ETA: 8:16 - loss: 2.669 - ETA: 8:12 - loss: 2.668 - ETA: 8:10 - loss: 2.668 - ETA: 8:07 - loss: 2.670 - ETA: 8:06 - loss: 2.671 - ETA: 8:01 - loss: 2.670 - ETA: 8:00 - loss: 2.669 - ETA: 8:01 - loss: 2.669 - ETA: 7:59 - loss: 2.669 - ETA: 7:57 - loss: 2.669 - ETA: 7:56 - loss: 2.668 - ETA: 7:54 - loss: 2.667 - ETA: 7:52 - loss: 2.666 - ETA: 7:50 - loss: 2.666 - ETA: 7:47 - loss: 2.668 - ETA: 7:44 - loss: 2.666 - ETA: 7:44 - loss: 2.666 - ETA: 7:43 - loss: 2.665 - ETA: 7:41 - loss: 2.666 - ETA: 7:37 - loss: 2.665 - ETA: 7:36 - loss: 2.664 - ETA: 7:34 - loss: 2.663 - ETA: 7:32 - loss: 2.662 - ETA: 7:32 - loss: 2.660 - ETA: 7:29 - loss: 2.660 - ETA: 7:26 - loss: 2.660 - ETA: 7:25 - loss: 2.660 - ETA: 7:23 - loss: 2.659 - ETA: 7:20 - loss: 2.658 - ETA: 7:17 - loss: 2.657 - ETA: 7:15 - loss: 2.657 - ETA: 7:13 - loss: 2.656 - ETA: 7:12 - loss: 2.655 - ETA: 7:08 - loss: 2.655 - ETA: 7:07 - loss: 2.654 - ETA: 7:04 - loss: 2.653 - ETA: 7:03 - loss: 2.652 - ETA: 7:01 - loss: 2.653 - ETA: 7:01 - loss: 2.652 - ETA: 6:58 - loss: 2.652 - ETA: 6:56 - loss: 2.652 - ETA: 6:55 - loss: 2.651 - ETA: 6:52 - loss: 2.650 - ETA: 6:49 - loss: 2.649 - ETA: 6:46 - loss: 2.649 - ETA: 6:44 - loss: 2.648 - ETA: 6:41 - loss: 2.647 - ETA: 6:38 - loss: 2.647 - ETA: 6:38 - loss: 2.648 - ETA: 6:37 - loss: 2.647 - ETA: 6:36 - loss: 2.646 - ETA: 6:35 - loss: 2.645 - ETA: 6:33 - loss: 2.645 - ETA: 6:30 - loss: 2.644 - ETA: 6:29 - loss: 2.643 - ETA: 6:28 - loss: 2.642 - ETA: 6:25 - loss: 2.643 - ETA: 6:23 - loss: 2.643 - ETA: 6:21 - loss: 2.642 - ETA: 6:19 - loss: 2.641 - ETA: 6:18 - loss: 2.641 - ETA: 6:15 - loss: 2.641 - ETA: 6:12 - loss: 2.641 - ETA: 6:11 - loss: 2.640 - ETA: 6:09 - loss: 2.640 - ETA: 6:09 - loss: 2.640 - ETA: 6:08 - loss: 2.639 - ETA: 6:06 - loss: 2.639 - ETA: 6:04 - loss: 2.638 - ETA: 6:02 - loss: 2.637 - ETA: 6:00 - loss: 2.637 - ETA: 5:59 - loss: 2.6368"
     ]
    }
   ],
   "source": [
    "# TODO: Set the epochs to train for.\n",
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 10\n",
    "steps_per_epoch = 300\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the predictions.\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Make predictions.\n",
    "\n",
    "y_pred = model.predict(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw prediction `y_pred`\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.3,\n",
    "                                   iou_threshold=0.1,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'Varroa']\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to know the fucking parameters on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_ic[5].shape)\n",
    "print(val_xmls[5])\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(val_ic[5])  \n",
    "rect = patches.Rectangle((val_xmls[5][0]['bbox'][0], val_xmls[5][0]['bbox'][1]), val_xmls[5][0]['bbox'][2], val_xmls[5][0]['bbox'][3],\n",
    "                linewidth=1,edgecolor='r',facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape all the images to 1024x1024 and conserv aspect ratio\n",
    "from skimage.transform import resize\n",
    "def reshape_img_bb(img,bbox_list):\n",
    "    DIM_IMG = 1024\n",
    "    \n",
    "    ratio_x = DIM_IMG/img.shape[1]\n",
    "    ratio_y = DIM_IMG/img.shape[0]\n",
    "    result_img = resize(img,(DIM_IMG,DIM_IMG),anti_aliasing=True)\n",
    "    result_boxes = []\n",
    "    if 0 != len(bbox_list):\n",
    "        for elem in bbox_list:\n",
    "            new_x = int(elem['bbox'][0] * ratio_x)\n",
    "            new_y = int(elem['bbox'][1] * ratio_y)\n",
    "            new_w = int(elem['bbox'][2] * ratio_x)\n",
    "            new_h = int(elem['bbox'][3] * ratio_x)\n",
    "            result_boxes.append((new_x,new_y,new_w,new_h))\n",
    "            \n",
    "    return (result_img,result_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fucking validate the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(121)\n",
    "for elem in tst_xmls[2]:\n",
    "    rect = patches.Rectangle((elem['bbox'][0], elem['bbox'][1]), elem['bbox'][2], elem['bbox'][3],\n",
    "                    linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "ax.imshow(tst_ic[2])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "print(tst_ic[2].shape)\n",
    "print(tst_xmls[2])\n",
    "n_img,n_boxes = reshape_img_bb(tst_ic[2],tst_xmls[2])\n",
    "\n",
    "for elem in n_boxes:\n",
    "    rect = patches.Rectangle((elem[0], elem[1]), elem[2], elem[3],\n",
    "                    linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "ax.imshow(n_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add SDD7 support and make it fucking work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(200, input_dim=64), \n",
    "        Activation('relu'), \n",
    "        Dropout(0.2), \n",
    "        Dense(4)\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You can generate a json submission file by using the function ''**generate_pred_json**''. This prediction file can be uploaded online for evaluation (Please refer to section 3 of the project description for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def generate_pred_json(data, tag='baseline'):\n",
    "    '''\n",
    "    Input\n",
    "    - data: Is a dictionary d, such that:\n",
    "          d = { \n",
    "              \"ID_1\": [], \n",
    "              \"ID_2\": [[x_21, y_21, w_21, h_21], [x_22, y_22, w_22, h_22]], \n",
    "              ... \n",
    "              \"ID_i\": [[x_i1, y_i1, w_i1, h_i1], ..., [x_iJ, y_iJ, w_iJ, h_iJ]],\n",
    "              ... \n",
    "              \"ID_N\": [[x_N1, y_N1, w_N1, h_N1]],\n",
    "          }\n",
    "          where ID is the string id of the image (e.i. 5a05e86fa07d56baef59b1cb_32.00px_1) and the value the Kx4 \n",
    "          array of intergers for the K predicted bounding boxes (e.g. [[170, 120, 15, 15]])\n",
    "    - tag: (optional) string that will be added to the name of the json file.\n",
    "    Output\n",
    "      Create a json file, \"prediction_[tag].json\", conatining the prediction to EvalAI format.\n",
    "    '''\n",
    "    unvalid_key = []\n",
    "    _data = data.copy()\n",
    "    for key, value in _data.items():\n",
    "        try:\n",
    "            # Try to convert to numpy array and cast as closest int\n",
    "            print(key)\n",
    "            v = np.around(np.array(value)).astype(int)\n",
    "            # Check is it is a 2d array with 4 columns (x,y,w,h)\n",
    "            if v.ndim != 2 or v.shape[1] != 4:\n",
    "                unvalid_key.append(key)\n",
    "            # Id must be a string\n",
    "            if not isinstance(key, str):\n",
    "                unvalid_key.append(key)\n",
    "            _data[key] = v.tolist()\n",
    "        # Deal with not consistant array size and empty predictions\n",
    "        except (ValueError, TypeError):\n",
    "            unvalid_key.append(key)\n",
    "    # Remove unvalid key from dictionnary\n",
    "    for key in unvalid_key: del _data[key]\n",
    "    \n",
    "    with open('prediction_{}.json'.format(tag), 'w') as outfile:\n",
    "        json.dump(_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
